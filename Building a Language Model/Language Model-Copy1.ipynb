{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f87b502",
   "metadata": {},
   "source": [
    "# <u> ICS2203 – Statistical Natural Language Processing <u>\n",
    "## <u> Building a Language Model – Part I <u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6d96e4",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad830519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard library imports\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "# third-party library imports\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer, sent_tokenize, word_tokenize\n",
    "from spellchecker import SpellChecker\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xml.etree.ElementTree as ET\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952f74c8",
   "metadata": {},
   "source": [
    "## Extracting and Preprocessing the Selected Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fdbeb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1:\n",
      "oxford art journal .\n",
      "\n",
      "Sentence 2:\n",
      "sample containing word periodical domain art data capture transcription oxford university press bnc xml edition december token w-units s-units distributed licence oxford university computing service behalf bnc consortium .\n",
      "\n",
      "Sentence 3:\n",
      "material protected international copyright law may copied redistributed way .\n",
      "\n",
      "Sentence 4:\n",
      "consult bnc web site full licencing distribution condition .\n",
      "\n",
      "Sentence 5:\n",
      "au artjnl oxford art journal .\n",
      "\n",
      "Sentence 6:\n",
      "oxford university press oxford w achumanities art art tag usage updated bnc-xml last check bnc world first release redo tagusage table check tagcounts resequenced s-units added header added date info updated catrefs updated source title updated title corrected tagusage po code revised bnc- header updated initial accession corpus drawn image guy brett certain image matter one desire answer question involuntary response ?\n",
      "\n",
      "Sentence 7:\n",
      "seem important answer ‘ objective ’ quality insight history society knowledge rather point merely personal obsession ?\n",
      "\n",
      "Sentence 8:\n",
      "american film-maker maya deren say somewhere ‘ response always precede analysis ’ remark sound made artist 's challenge academic dryness formalism .\n",
      "\n",
      "Sentence 9:\n",
      "response analysis actually part phenomenon whether result exclamation ‘ beautiful ! '\n",
      "\n",
      "Sentence 10:\n",
      "new ‘ reading ’ ?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus_dir_path = 'corpus/aca'\n",
    "sentences = []\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "generation_start = datetime.now()\n",
    "\n",
    "for root_dir, subdirs, files in os.walk(corpus_dir_path):\n",
    "    for filename in files:\n",
    "        if filename.endswith('.xml'):\n",
    "            tree = ET.parse(os.path.join(root_dir, filename))\n",
    "            root = tree.getroot()\n",
    "            text = ''\n",
    "            for element in root.iter():\n",
    "                if element.text is not None:\n",
    "                    # Concatenate text from all elements in the file\n",
    "                    text += element.text + ' '\n",
    "\n",
    "            # Remove URLs, numbers, brackets, commas, semicolons, and colons\n",
    "            text = re.sub(r'https?://\\S+|www\\.\\S+|\\d+|[{};:\\[\\],()]', '', text)\n",
    "            \n",
    "            # Convert text to lowercase and split into sentences\n",
    "            text = text.lower()\n",
    "            file_sentences = sent_tokenize(text)\n",
    "\n",
    "            # Process each sentence\n",
    "            for sentence in file_sentences:\n",
    "                # Tokenize sentence into words\n",
    "                sentence_tokens = word_tokenize(sentence)\n",
    "\n",
    "                # Remove stopwords, numbers, and lemmatize tokens\n",
    "                sentence_tokens = [lemmatizer.lemmatize(token) for token in sentence_tokens if token not in stop_words and not token.isdigit()]\n",
    "\n",
    "                # Append lemmatized tokens to list of sentences\n",
    "                sentences.append(sentence_tokens)\n",
    "\n",
    "generation_end = datetime.now()\n",
    "generation_time = generation_end - generation_start\n",
    "\n",
    "def RAMusage():\n",
    "    pid = os.getpid()\n",
    "    py = psutil.Process(pid)\n",
    "    memoryUse = py.memory_info()[0] / 2.**30\n",
    "    return memoryUse\n",
    "\n",
    "# Output list of the first 50 sentences\n",
    "# print(sentences[:50])\n",
    "\n",
    "# Output the first 50 sentences and their words\n",
    "for i, sentence in enumerate(sentences[:10]):\n",
    "    print(f\"Sentence {i+1}:\")\n",
    "    print(\" \".join(sentence))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffae0ee0",
   "metadata": {},
   "source": [
    "## Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67fe436e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of corpus: 587467 words, 40214 sentences\n",
      "Generation Time (HH:MM:SS:ms): 0:00:18.489743\n",
      "\n",
      "Memory Usage: 0.259476 GB\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total number of sentences in the corpus\n",
    "num_sentences = len(sentences)\n",
    "\n",
    "# Calculate the total number of words in the corpus\n",
    "num_words = sum([len(doc) for doc in sentences])\n",
    "\n",
    "# Print the total size of the corpus in sentences and words\n",
    "print(\"Total size of corpus: {} words, {} sentences\".format(num_words, num_sentences))\n",
    "\n",
    "# Print the time taken to generate the tokens\n",
    "print('Generation Time (HH:MM:SS:ms): {}\\n'.format(generation_time))\n",
    "\n",
    "# Print the current memory usage of the program\n",
    "print(\"Memory Usage: {:.6f} GB\".format(RAMusage()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22f0a11",
   "metadata": {},
   "source": [
    "## Split the corpus tokens into test set and train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54d92c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 32171\n",
      "Test set size: 8043\n"
     ]
    }
   ],
   "source": [
    "# Split the sentences into train and test sets\n",
    "train_set, test_set = train_test_split(sentences, test_size=0.2)\n",
    "\n",
    "# Define the vocabulary from the training set\n",
    "vocab = set()\n",
    "for sentence in train_set:\n",
    "    for word in sentence:\n",
    "        # Add each unique word in the training set to the vocabulary\n",
    "        vocab.add(word)\n",
    "# Convert the vocabulary to a list and sort it alphabetically\n",
    "vocab = list(set([word for sentence in train_set for word in sentence]))\n",
    "vocab.sort()\n",
    "\n",
    "# Print the size of the train and test sets\n",
    "print(\"Train set size:\", len(train_set))\n",
    "print(\"Test set size:\", len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f8f767",
   "metadata": {},
   "source": [
    "## Building frequency counts for n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5eca38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngram_counts(sentences, n):\n",
    "    # Initialize a dictionary to store the frequency counts\n",
    "    ngram_counts = {}\n",
    "\n",
    "    # Iterate through each sentence in the corpus\n",
    "    for sentence in sentences:\n",
    "        # Create n-grams from the words in the sentence\n",
    "        ngrams = [tuple(sentence[i:i+n]) for i in range(len(sentence)-n+1)]\n",
    "\n",
    "        # Iterate through each n-gram in the sentence\n",
    "        for ngram in ngrams:\n",
    "            # Update the frequency count for the n-gram\n",
    "            if ngram in ngram_counts:\n",
    "                ngram_counts[ngram] += 1\n",
    "            else:\n",
    "                ngram_counts[ngram] = 1\n",
    "\n",
    "    return ngram_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef97c870",
   "metadata": {},
   "source": [
    "## Vanilla Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99a139dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vanilla Language Model\n",
    "\n",
    "unigram_counts_vanilla = generate_ngram_counts(sentences, 1)\n",
    "#print(\"Unigram Counts:\", unigram_counts_vanilla)\n",
    "\n",
    "bigram_counts_vanilla = generate_ngram_counts(sentences, 2)\n",
    "#print(\"Bigram Counts:\", bigram_counts_vanilla)\n",
    "\n",
    "trigram_counts_vanilla = generate_ngram_counts(sentences, 3)\n",
    "#print(\"Trigram Counts:\", trigram_counts_vanilla)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e399bd3c",
   "metadata": {},
   "source": [
    "## Laplace Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "067ebf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Laplace Language Model\n",
    "\n",
    "def laplace_language_model(vocab, unigram_counts_vanilla, bigram_counts_vanilla, trigram_counts_vanilla):\n",
    "    # Define vocabulary size\n",
    "    V = len(vocab)\n",
    "\n",
    "    # Initialize Laplace-smoothed count dictionaries\n",
    "    unigram_counts_smoothed = {}\n",
    "    bigram_counts_smoothed = {}\n",
    "    trigram_counts_smoothed = {}\n",
    "\n",
    "    # Set the smoothing parameter\n",
    "    k = 1\n",
    "\n",
    "    # Laplace-smooth the unigram counts\n",
    "    for word in vocab:\n",
    "        if word in unigram_counts_vanilla:\n",
    "            unigram_counts_smoothed[word] = unigram_counts_vanilla[word] + k\n",
    "        else:\n",
    "            unigram_counts_smoothed[word] = k\n",
    "\n",
    "    # Laplace-smooth the bigram counts\n",
    "    for bigram in bigram_counts_vanilla:\n",
    "        if bigram[0] in unigram_counts_vanilla:\n",
    "            bigram_counts_smoothed[bigram] = (bigram_counts_vanilla[bigram] + k) / (unigram_counts_vanilla[bigram[0]] + V*k)\n",
    "        else:\n",
    "            bigram_counts_smoothed[bigram] = k / V\n",
    "\n",
    "    # Laplace-smooth the trigram counts\n",
    "    for trigram in trigram_counts_vanilla:\n",
    "        if (trigram[0], trigram[1]) in bigram_counts_vanilla:\n",
    "            trigram_counts_smoothed[trigram] = (trigram_counts_vanilla[trigram] + k) / (bigram_counts_vanilla[(trigram[0], trigram[1])] + V*k)\n",
    "        else:\n",
    "            trigram_counts_smoothed[trigram] = k / V**2\n",
    "\n",
    "    return unigram_counts_smoothed, bigram_counts_smoothed, trigram_counts_smoothed\n",
    "\n",
    "# Build the Laplace Language Model\n",
    "unigram_counts_laplace, bigram_counts_laplace, trigram_counts_laplace = laplace_language_model(vocab, unigram_counts_vanilla, bigram_counts_vanilla, trigram_counts_vanilla)\n",
    "\n",
    "# Print the Laplace-smoothed trigram counts\n",
    "#print(\"\\nLaplace-smooothed Language Model:\")\n",
    "#print(\"Unigram Counts:\", unigram_counts_laplace)\n",
    "#print(\"Bigram Counts:\", bigram_counts_laplace)\n",
    "#print(\"Trigram Counts:\", trigram_counts_laplace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d433cf79",
   "metadata": {},
   "source": [
    "## UNK Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e6a2797",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNK Vocabulary size: 16514\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def replace_rare_words(sentences, threshold):\n",
    "    # Create a frequency distribution of all words in the corpus\n",
    "    word_freq = Counter([word for sentence in sentences for word in sentence])\n",
    "\n",
    "    # Identify words that occur less than the threshold number of times\n",
    "    rare_words = set([word for word in word_freq if word_freq[word] < threshold])\n",
    "\n",
    "    # Replace rare words with the UNK token\n",
    "    new_sentences = []\n",
    "    for sentence in sentences:\n",
    "        new_sentence = []\n",
    "        for word in sentence:\n",
    "            if word in rare_words:\n",
    "                new_sentence.append('<UNK>')\n",
    "            else:\n",
    "                new_sentence.append(word)\n",
    "        new_sentences.append(new_sentence)\n",
    "\n",
    "    return new_sentences\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Replace rare words in the training set with the UNK token\n",
    "train_set_unk = replace_rare_words(train_set, threshold=2)\n",
    "\n",
    "# Replace rare words in the test set with the UNK token\n",
    "test_set_unk = replace_rare_words(test_set, threshold=2)\n",
    "\n",
    "# Define the UNK vocabulary from the training set\n",
    "vocab_unk = set()\n",
    "for sentence in train_set_unk:\n",
    "    for word in sentence:\n",
    "        # Add each unique word in the training set to the vocabulary\n",
    "        vocab_unk.add(word)\n",
    "# Convert the vocabulary to a list and sort it alphabetically\n",
    "vocab_unk = list(vocab_unk)\n",
    "vocab_unk.sort()\n",
    "\n",
    "# Print the size of the UNK vocabulary\n",
    "print(\"UNK Vocabulary size:\", len(vocab_unk))\n",
    "print(\"-\" *125)\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def unk_language_model(vocab_unk, train_set_unk):\n",
    "    # Define vocabulary size\n",
    "    V = len(vocab_unk)\n",
    "\n",
    "    # Initialize count dictionaries\n",
    "    unigram_counts_unk = {}\n",
    "    bigram_counts_unk = {}\n",
    "    trigram_counts_unk = {}\n",
    "\n",
    "    # Set the smoothing parameter\n",
    "    k = 1\n",
    "\n",
    "    # Generate n-gram counts for the UNK Language Model\n",
    "    unigram_counts_unk = generate_ngram_counts(train_set_unk, 1)\n",
    "    bigram_counts_unk = generate_ngram_counts(train_set_unk, 2)\n",
    "    trigram_counts_unk = generate_ngram_counts(train_set_unk, 3)\n",
    "\n",
    "    # Laplace-smooth the unigram counts\n",
    "    for word in vocab_unk:\n",
    "        if word in unigram_counts_unk:\n",
    "            unigram_counts_unk[word] += k\n",
    "        else:\n",
    "            unigram_counts_unk[word] = k\n",
    "\n",
    "    # Laplace-smooth the bigram counts\n",
    "    for bigram in bigram_counts_unk:\n",
    "        if bigram[0] in unigram_counts_unk:\n",
    "            bigram_counts_unk[bigram] = (bigram_counts_unk[bigram] + k) / (unigram_counts_unk[bigram[0]] + V*k)\n",
    "        else:\n",
    "            bigram_counts_unk[bigram] = k / V\n",
    "\n",
    "    # Laplace-smooth the trigram counts\n",
    "    for trigram in trigram_counts_unk:\n",
    "        if (trigram[0], trigram[1]) in bigram_counts_unk:\n",
    "            trigram_counts_unk[trigram] = (trigram_counts_unk[trigram] + k) / (bigram_counts_unk[(trigram[0], trigram[1])] + V*k)\n",
    "        else:\n",
    "            trigram_counts_unk[trigram] = k / V**2\n",
    "\n",
    "    return unigram_counts_unk, bigram_counts_unk, trigram_counts_unk\n",
    "\n",
    "# Build the UNK Language Model\n",
    "unigram_counts_unk, bigram_counts_unk, trigram_counts_unk = unk_language_model(vocab_unk, train_set_unk)\n",
    "\n",
    "# Print the UNK Language Model counts\n",
    "#print(\"\\nUNK Language Model:\\n\")\n",
    "#print(\"Unigram Counts:\", unigram_counts_unk)\n",
    "#print(\"Bigram Counts:\", bigram_counts_unk)\n",
    "#print(\"Trigram Counts:\", trigram_counts_unk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4153f93",
   "metadata": {},
   "source": [
    "## Linear Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58801cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of sentence (trigram using the Laplace-smoothed counts): 6.691399023299299e-06\n"
     ]
    }
   ],
   "source": [
    "def linear_interpolation(sentence, unigram_counts, bigram_counts, trigram_counts, lambda1=0.1, lambda2=0.3, lambda3=0.6):\n",
    "    # Initialize the probability to 1\n",
    "    probability = 1.0\n",
    "\n",
    "    # Iterate through each trigram in the sentence\n",
    "    for i in range(2, len(sentence)):\n",
    "        trigram = (sentence[i-2], sentence[i-1], sentence[i])\n",
    "\n",
    "        # Calculate the interpolated probability of the trigram using the Laplace-smoothed counts\n",
    "        unigram_prob = unigram_counts.get(trigram[2], 0) / sum(unigram_counts.values())\n",
    "        bigram_prob = bigram_counts.get(trigram[1:], 0) / sum(bigram_counts.values())\n",
    "        trigram_prob = trigram_counts.get(trigram, 0) / sum(trigram_counts.values())\n",
    "        interpolated_prob = lambda1 * unigram_prob + lambda2 * bigram_prob + lambda3 * trigram_prob\n",
    "\n",
    "        # Multiply the probability by the interpolated probability of the trigram\n",
    "        probability *= interpolated_prob\n",
    "\n",
    "    return probability\n",
    "\n",
    "# Example sentence\n",
    "test_sentence = ['oxford', 'art', 'journal']\n",
    "\n",
    "# Calculate the probability of the test sentence using linear interpolation with the given lambdas\n",
    "prob = linear_interpolation(test_sentence, unigram_counts_laplace, bigram_counts_laplace, trigram_counts_laplace, lambda1=0.1, lambda2=0.3, lambda3=0.6)\n",
    "\n",
    "# Print the probability of the test sentence (trigram using the Laplace-smoothed counts)\n",
    "print(\"Probability of sentence (trigram using the Laplace-smoothed counts):\", prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3bb5f3",
   "metadata": {},
   "source": [
    "## Probabilities of Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "069267c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_probability(sentence, unigram_model):\n",
    "    tokens = sentence.split()\n",
    "    probability = 0\n",
    "    total_unigram_count = sum(unigram_model.values())\n",
    "\n",
    "    for token in tokens:\n",
    "        # Check if the token exists in the unigram model\n",
    "        if token in unigram_model:\n",
    "            token_prob = unigram_model[token] / total_unigram_count\n",
    "            probability += math.log(token_prob)\n",
    "        else:\n",
    "            # If the token is not in the unigram model, you can either skip it or assign a small probability value\n",
    "            pass\n",
    "\n",
    "    return probability\n",
    "\n",
    "def bigram_probability(sentence, bigram_model):\n",
    "    tokens = sentence.split()\n",
    "    probability = 0\n",
    "\n",
    "    for i in range(len(tokens) - 1):\n",
    "        bigram = (tokens[i], tokens[i + 1])\n",
    "        probability += math.log(bigram_model.get(bigram, 1e-10))\n",
    "\n",
    "    return probability\n",
    "\n",
    "\n",
    "def trigram_probability(sentence, trigram_model):\n",
    "    tokens = sentence.split()\n",
    "    probability = 0\n",
    "\n",
    "    for i in range(len(tokens) - 2):\n",
    "        trigram = (tokens[i], tokens[i + 1], tokens[i + 2])\n",
    "        probability += math.log(trigram_model.get(trigram, 1e-10))\n",
    "\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aafae60",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3509838",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: place emphasis biotic others environmental factor .\n",
      "Vanilla Model Probability of Sentence 1: 0.008547008547008546\n",
      "Laplace Model Probability of Sentence 1: 2.5226909574604952e-48\n",
      "UNK Model Probability of Sentence 1: 6.489163931678202e-46\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Sentence 2: ‘ trapline ’ moving directly one food site next apparently remembering previous day fast flier visiting plant producing flower long period .\n",
      "Vanilla Model Probability of Sentence 2: 4.4288548752834467e-05\n",
      "Laplace Model Probability of Sentence 2: 4.0501513289479383e-191\n",
      "UNK Model Probability of Sentence 2: 1.8074007068658315e-181\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Sentence 3: fact whole point introducing d. assigning relative permittivity dielectric material problem taken care .\n",
      "Vanilla Model Probability of Sentence 3: 0.00025720164609053495\n",
      "Laplace Model Probability of Sentence 3: 5.813497582446366e-115\n",
      "UNK Model Probability of Sentence 3: 3.5286925970993063e-109\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Sentence 4: discus .\n",
      "Vanilla Model Probability of Sentence 4: 1.0\n",
      "Laplace Model Probability of Sentence 4: 1.0\n",
      "UNK Model Probability of Sentence 4: 1.0\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Sentence 5: hegel 's conception historical time reflects conception intrinsic unity part social totality part whole whole present part history partakes self-reflective immediacy paradoxically make ahistorical .\n",
      "Vanilla Model Probability of Sentence 5: 1.0212418300653594e-05\n",
      "Laplace Model Probability of Sentence 5: 1.1182214359674907e-219\n",
      "UNK Model Probability of Sentence 5: 1.5407515919574094e-208\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Sentence 6: period run defendant 's act infliction damage .\n",
      "Vanilla Model Probability of Sentence 6: 0.00021645021645021645\n",
      "Laplace Model Probability of Sentence 6: 7.625422525157483e-58\n",
      "UNK Model Probability of Sentence 6: 5.961877010714002e-55\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Sentence 7: coll .\n",
      "Vanilla Model Probability of Sentence 7: 1.0\n",
      "Laplace Model Probability of Sentence 7: 1.0\n",
      "UNK Model Probability of Sentence 7: 1.0\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Sentence 8: alternatively low angle laser light scattering lalls instrument attached series concentration detector .\n",
      "Vanilla Model Probability of Sentence 8: 0.038461538461538464\n",
      "Laplace Model Probability of Sentence 8: 1.9234249746193884e-105\n",
      "UNK Model Probability of Sentence 8: 3.8474086493219955e-100\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Sentence 9: fossil specie rhus known tertiary rock .\n",
      "Vanilla Model Probability of Sentence 9: 0.0125\n",
      "Laplace Model Probability of Sentence 9: 2.522690905645183e-48\n",
      "UNK Model Probability of Sentence 9: 6.483476592706901e-46\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Sentence 10: evidently introduction unit reduces degeneracy — c unity .\n",
      "Vanilla Model Probability of Sentence 10: 0.009615384615384616\n",
      "Laplace Model Probability of Sentence 10: 2.304562752001009e-67\n",
      "UNK Model Probability of Sentence 10: 5.4444052760064886e-64\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def calculate_probs(test_set, model):\n",
    "    probs = []\n",
    "    for sentence in test_set:\n",
    "        probability = 1.0\n",
    "        for i in range(2, len(sentence)):\n",
    "            trigram = (sentence[i-2], sentence[i-1], sentence[i])\n",
    "            if model == \"vanilla\":\n",
    "                trigram_count = trigram_counts_vanilla.get(trigram, 0)\n",
    "                bigram_count = bigram_counts_vanilla.get(trigram[1:], 0)\n",
    "                if bigram_count != 0:\n",
    "                    probability *= trigram_count / bigram_count\n",
    "            elif model == \"laplace\":\n",
    "                trigram_count = trigram_counts_laplace.get(trigram, 0)\n",
    "                bigram_count = bigram_counts_laplace.get(trigram[1:], 0)\n",
    "                unigram_count = unigram_counts_laplace.get(trigram[2], 0)\n",
    "                probability *= (trigram_count + 1) / (bigram_count + len(vocab) + 1 * len(vocab))\n",
    "                probability *= (bigram_count + 1) / (unigram_count + len(vocab) + 1 * len(vocab))\n",
    "            elif model == \"unk\":\n",
    "                trigram_count = trigram_counts_unk.get(trigram, 0)\n",
    "                bigram_count = bigram_counts_unk.get(trigram[1:], 0)\n",
    "                unigram_count = unigram_counts_unk.get(trigram[2], 0)\n",
    "                probability *= (trigram_count + 1) / (bigram_count + len(vocab_unk) + 1 * len(vocab_unk))\n",
    "                probability *= (bigram_count + 1) / (unigram_count + len(vocab_unk) + 1 * len(vocab_unk))\n",
    "        probs.append(probability)\n",
    "    return probs\n",
    "\n",
    "def print_probs(vanilla_probs, laplace_probs, unk_probs, test_set, test_set_unk):\n",
    "    for i in range(10):\n",
    "        print(\"Sentence {}: {}\".format(i + 1, ' '.join(test_set[i])))\n",
    "        print(\"Vanilla Model Probability of Sentence {}: {}\".format(i + 1, vanilla_probs[i]))\n",
    "        print(\"Laplace Model Probability of Sentence {}: {}\".format(i + 1, laplace_probs[i]))\n",
    "        print(\"UNK Model Probability of Sentence {}: {}\".format(i + 1, unk_probs[i]))\n",
    "        print(\"-\" * 125)\n",
    "\n",
    "# Calculate the probabilities for each model\n",
    "vanilla_probs = calculate_probs(test_set, \"vanilla\")\n",
    "laplace_probs = calculate_probs(test_set, \"laplace\")\n",
    "unk_probs = calculate_probs(test_set_unk, \"unk\")\n",
    "\n",
    "# Print the probabilities\n",
    "print_probs(vanilla_probs, laplace_probs, unk_probs, test_set, test_set_unk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cea728",
   "metadata": {},
   "source": [
    "## Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db9eb574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model           | Unigram   | Bigram    | Trigram\n",
      "-------------------------------------------------\n",
      "Vanilla         |    1.00  |    0.50 |    0.91\n",
      "Laplace         | 21761.65 | 14262.84 | 3645.54\n",
      "UNK             | 422616.68 | 3014759.52 | 143104084.28\n"
     ]
    }
   ],
   "source": [
    "def calculate_perplexity(test_set, unigram_model, bigram_model, trigram_model):\n",
    "    model_perplexities = []\n",
    "\n",
    "    for model_probability in [unigram_probability, bigram_probability, trigram_probability]:\n",
    "        probability_sum = 0\n",
    "        n = 0\n",
    "\n",
    "        for sentence in test_set:\n",
    "            sent = ' '.join(sentence)\n",
    "            probability_sum += model_probability(sent, unigram_model if model_probability == unigram_probability else bigram_model if model_probability == bigram_probability else trigram_model)\n",
    "            n += len(sentence)\n",
    "\n",
    "        model_perplexity = math.exp(-probability_sum / n)\n",
    "        model_perplexities.append(model_perplexity)\n",
    "\n",
    "    return tuple(model_perplexities)\n",
    "\n",
    "vanilla_perplexities = calculate_perplexity(test_set, unigram_counts_vanilla, bigram_counts_vanilla, trigram_counts_vanilla)\n",
    "laplace_perplexities = calculate_perplexity(test_set, unigram_counts_laplace, bigram_counts_laplace, trigram_counts_laplace)\n",
    "unk_perplexities = calculate_perplexity(test_set_unk, unigram_counts_unk, bigram_counts_unk, trigram_counts_unk)\n",
    "\n",
    "print(\"Model           | Unigram   | Bigram    | Trigram\")\n",
    "print(\"-------------------------------------------------\")\n",
    "print(f\"Vanilla         | {vanilla_perplexities[0]:7.2f}  | {vanilla_perplexities[1]:7.2f} | {vanilla_perplexities[2]:7.2f}\")\n",
    "print(f\"Laplace         | {laplace_perplexities[0]:7.2f} | {laplace_perplexities[1]:7.2f} | {laplace_perplexities[2]:7.2f}\")\n",
    "print(f\"UNK             | {unk_perplexities[0]:7.2f} | {unk_perplexities[1]:7.2f} | {unk_perplexities[2]:7.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c44db85",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23965b4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which language model would you like to use? (Vanilla, Laplace, UNK): Vanilla\n",
      "Please enter a phrase: they all\n",
      "Generating Vanilla model...\n",
      "GENERATED VANILLA SENTENCES:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "a must be 1-dimensional",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25168\\3769307023.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;31m# Generate the sentence using the selected model and input phrase\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m \u001b[0mgenerate_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphrase_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25168\\3769307023.py\u001b[0m in \u001b[0;36mgenerate_sentence\u001b[1;34m(model, phrase)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"GENERATED\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"SENTENCES:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mngram_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"unigram\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"bigram\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"trigram\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mgenerated_sentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_ngram_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphrase\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mngram_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{ngram_type.capitalize()}: {generated_sentence}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25168\\3769307023.py\u001b[0m in \u001b[0;36mgenerate_ngram_sentence\u001b[1;34m(model, phrase, ngram_type)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mend_token\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mngram_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"unigram\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m             \u001b[0mnext_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_counts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mngram_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"bigram\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[0mnext_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_counts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25168\\3769307023.py\u001b[0m in \u001b[0;36mgenerate_word\u001b[1;34m(model_count, context)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[0mcounts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcounts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[0mprobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcounts\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mcounts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m     \u001b[0mnext_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnext_word\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: a must be 1-dimensional"
     ]
    }
   ],
   "source": [
    "def generate_sentence(model, phrase):\n",
    "    print(f\"Generating {model} model...\")\n",
    "    print(\"GENERATED\", model.upper(), \"SENTENCES:\")\n",
    "    for ngram_type in [\"unigram\", \"bigram\", \"trigram\"]:\n",
    "        generated_sentence = generate_ngram_sentence(model, phrase, ngram_type)\n",
    "        print(f\"{ngram_type.capitalize()}: {generated_sentence}\")\n",
    "\n",
    "def generate_ngram_sentence(model, phrase, ngram_type):\n",
    "    start_token = '<s>'\n",
    "    end_token = '</s>'\n",
    "    max_length = 20\n",
    "    sentence = [start_token] + phrase.split()\n",
    "    model_counts = None\n",
    "\n",
    "    if model == 'Vanilla':\n",
    "        model_counts = [unigram_counts_vanilla, bigram_counts_vanilla, trigram_counts_vanilla]\n",
    "    elif model == 'Laplace':\n",
    "        model_counts = [unigram_counts_laplace, bigram_counts_laplace, trigram_counts_laplace]\n",
    "    elif model == 'UNK':\n",
    "        model_counts = [unigram_counts_unk, bigram_counts_unk, trigram_counts_unk]\n",
    "    else:\n",
    "        print('Invalid model selection')\n",
    "        return\n",
    "\n",
    "    while sentence[-1] != end_token and len(sentence) < max_length:\n",
    "        if ngram_type == \"unigram\":\n",
    "            next_word = generate_word(model_counts[0])\n",
    "        elif ngram_type == \"bigram\":\n",
    "            next_word = generate_word(model_counts[1], sentence[-1:])\n",
    "        elif ngram_type == \"trigram\":\n",
    "            if len(sentence) < 2:\n",
    "                next_word = generate_word(model_counts[1], sentence[-1:])\n",
    "            else:\n",
    "                next_word = generate_word(model_counts[2], sentence[-2:])\n",
    "        sentence.append(next_word)\n",
    "\n",
    "    return ' '.join(sentence[1:])\n",
    "\n",
    "def generate_word(model_count, context=None):\n",
    "    if context is None:\n",
    "        words, counts = zip(*[(word, count) for word, count in model_count.items()])\n",
    "    else:\n",
    "        vocab = sorted(model_count.keys())  # Sort vocab for consistent ordering\n",
    "        words_and_counts = []\n",
    "        for word in vocab:\n",
    "            key = tuple(context + [word])\n",
    "            if key in model_count:\n",
    "                count = model_count[key]\n",
    "                words_and_counts.append((word, count))\n",
    "\n",
    "        if len(words_and_counts) == 0:  # Check if there are no words to choose from\n",
    "            return '</s>'\n",
    "\n",
    "        words, counts = zip(*words_and_counts)\n",
    "        \n",
    "    words = list(words)  # Convert words to a list from a tuple\n",
    "    counts = np.array(counts, dtype=np.float64)\n",
    "    epsilon = 1e-10\n",
    "    counts = np.add(counts, epsilon)\n",
    "    probs = counts / counts.sum()\n",
    "    next_word = np.random.choice(words, p=probs)\n",
    "    return next_word\n",
    "\n",
    "# Ask the user for the model and phrase input\n",
    "model_input = input('Which language model would you like to use? (Vanilla, Laplace, UNK): ')\n",
    "phrase_input = input('Please enter a phrase: ')\n",
    "\n",
    "# Generate the sentence using the selected model and input phrase\n",
    "generate_sentence(model_input, phrase_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254de71d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
