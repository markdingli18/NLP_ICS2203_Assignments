{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f87b502",
   "metadata": {},
   "source": [
    "#### <u> Mark Dingli - 20703H <u>\n",
    "# <u> ICS2203 – Statistical Natural Language Processing <u>\n",
    "# <u> Building a Language Model – Part I <u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb462ad",
   "metadata": {},
   "source": [
    "#### This Jupyter notebook draws inspiration from the content presented in:\n",
    "\n",
    "J. Martin, and D. Jurafsky, \"Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition, 3rd ed.,\" Stanford University, 2020. [Online]. Available: https://web.stanford.edu/~jurafsky/slp3/3.pdf. [Accessed: March 10, 2023]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6d96e4",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad830519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library Imports\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "import xml.etree.ElementTree as ET\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "# Third-Party Library Imports\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# System Library Imports\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952f74c8",
   "metadata": {},
   "source": [
    "## Extracting and Pre-processing the Selected Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7dfd6b",
   "metadata": {},
   "source": [
    "#### The corpus used in this project is the (Baby) British National Corpus: http://ota.ox.ac.uk/desc/2553"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fdbeb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['oxford', 'art', 'journal', 'sample', 'containing', 'about', 'word', 'from', 'a', 'periodical', 'domain', 'art', 'data', 'capture', 'and', 'transcription', 'oxford', 'university', 'press', 'bnc', 'xml', 'edition', 'december', 'token', 'w-units', 's-units', 'distributed', 'under', 'licence', 'by', 'oxford', 'university', 'computing', 'service', 'on', 'behalf', 'of', 'the', 'bnc', 'consortium', 'this', 'material', 'is', 'protected', 'by', 'international', 'copyright', 'law', 'and', 'may', 'not', 'be', 'copied', 'or', 'redistributed', 'in', 'any', 'way', 'consult', 'the', 'bnc', 'web', 'site', 'at', 'for', 'full', 'licencing', 'and', 'distribution', 'condition', 'a6u', 'artjnl', 'oxford', 'art', 'journal', 'oxford', 'university', 'press', 'oxford', 'w', 'ac', 'humanity', 'art', 'art', 'tag', 'usage', 'updated', 'for', 'bnc-xml', 'last', 'check', 'for', 'bnc', 'world', 'first', 'release', 'redo', 'tagusage', 'table', 'check']\n"
     ]
    }
   ],
   "source": [
    "# Set directory path for corpus\n",
    "corpus_dir_path = 'corpus'\n",
    "\n",
    "# Create an empty list for tokens\n",
    "tokens = []\n",
    "\n",
    "# Instantiate a WordNetLemmatizer object\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Instantiate a RegexpTokenizer object for tokenizing text\n",
    "tokenizer = RegexpTokenizer(r\"\\w+(?:[-']\\w+)*|[^\\w\\s]\")\n",
    "\n",
    "# Traverse all files in the specified directory and subdirectories, and tokenize text in them\n",
    "for root_dir, subdirs, files in os.walk(corpus_dir_path):\n",
    "    for filename in files:\n",
    "        # Select only XML files\n",
    "        if filename.endswith('.xml'):\n",
    "            # Parse XML file\n",
    "            tree = ET.parse(os.path.join(root_dir, filename))\n",
    "            root = tree.getroot()\n",
    "            text = ''\n",
    "            for element in root.iter():\n",
    "                if element.text is not None:\n",
    "                    text += element.text + ' '\n",
    "\n",
    "            # Remove URLs from text\n",
    "            text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  \n",
    "\n",
    "            # Convert text to lowercase\n",
    "            text = text.lower()\n",
    "\n",
    "            # Tokenize text using RegexpTokenizer object\n",
    "            file_tokens = tokenizer.tokenize(text)\n",
    "\n",
    "            # Remove punctuation, numbers, and lemmatize tokens\n",
    "            file_tokens = [lemmatizer.lemmatize(token) for token in file_tokens \n",
    "                           if not token.isdigit() and \n",
    "                           (token.isalpha() or len(token) > 1)]\n",
    "\n",
    "            # Add the tokens to the tokens list\n",
    "            tokens.extend(file_tokens)\n",
    "\n",
    "# Output list of the first 100 tokens\n",
    "print()\n",
    "print(tokens[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b94acd",
   "metadata": {},
   "source": [
    "### Printing the size of list that includes all the words after preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67fe436e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of corpus: 4003540 words\n"
     ]
    }
   ],
   "source": [
    "# Print the total size of the token list\n",
    "print(\"Total size of corpus: \"+str(len(tokens))+\" words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22f0a11",
   "metadata": {},
   "source": [
    "## Splitting the corpus tokens into test set and train set\n",
    "\n",
    "I chose to make the test set 20% of the total corpus but that can easily be changed by adjusting the second variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54d92c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 3202832\n",
      "Test set size: 800708\n"
     ]
    }
   ],
   "source": [
    "# Split the words into train and test sets\n",
    "train_set, test_set = train_test_split(tokens, test_size=0.2)\n",
    "\n",
    "# Define the vocabulary from the training set\n",
    "vocab = set()\n",
    "for sentence in train_set:\n",
    "    for word in sentence.split():\n",
    "        vocab.add(word)\n",
    "vocab = list(vocab)\n",
    "vocab.sort()\n",
    "\n",
    "# Print the size of the train and test sets\n",
    "print(\"Train set size:\", len(train_set))\n",
    "print(\"Test set size:\", len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929102fb",
   "metadata": {},
   "source": [
    "# <u> Building a Language Model – Part II <u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99525c2d",
   "metadata": {},
   "source": [
    "## Vanilla Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7200d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vanilla Language Model Build & Export Time(HH:MM:SS:ms) - 0:00:38.731073\n",
      "\n",
      "Vanilla Language Model Memory Use: 1.965645 GB\n",
      "\n",
      "Vanilla Unigram Model Size: 2.20 MB\n",
      "Vanilla Bigram Model Size: 58.39 MB\n",
      "Vanilla Trigram Model Size: 113.48 MB\n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Define a function to get RAM usage\n",
    "def RAMusage():\n",
    "    pid = os.getpid()\n",
    "    py = psutil.Process(pid)\n",
    "    memoryUse = py.memory_info()[0] / 2.**30\n",
    "    return memoryUse\n",
    "\n",
    "# Record start time for model export\n",
    "vanilla_export_start = datetime.now()\n",
    "\n",
    "# Define function to create n-grams from a list of tokens\n",
    "def create_vanilla_ngrams(tokens, n):\n",
    "    return [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Create a vanilla unigram model by iterating over the train_set and counting the frequency of each word\n",
    "vanilla_unigram_freq = {}\n",
    "vanilla_total_unigrams = len(train_set)\n",
    "for word in train_set:\n",
    "    if word in vanilla_unigram_freq:\n",
    "        vanilla_unigram_freq[word] += 1\n",
    "    else:\n",
    "        vanilla_unigram_freq[word] = 1\n",
    "\n",
    "# Calculate vanilla unigram probabilities by dividing the frequency of each word by the total number of unigrams\n",
    "vanilla_unigram_probs = {}\n",
    "for word, count in vanilla_unigram_freq.items():\n",
    "    vanilla_unigram_probs[word] = count / vanilla_total_unigrams\n",
    "    \n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Create a vanilla bigram model by creating bigrams from the train_set and counting the frequency of each bigram\n",
    "vanilla_bigram_freq = {}\n",
    "vanilla_total_bigrams = len(create_vanilla_ngrams(train_set, 2))\n",
    "vanilla_bigram_tokens = create_vanilla_ngrams(train_set, 2)\n",
    "for tokens in vanilla_bigram_tokens:\n",
    "    if tokens in vanilla_bigram_freq:\n",
    "        vanilla_bigram_freq[tokens] += 1\n",
    "    else:\n",
    "        vanilla_bigram_freq[tokens] = 1\n",
    "\n",
    "# Calculate vanilla bigram probabilities by dividing the frequency of each bigram by the frequency of its first word\n",
    "vanilla_bigram_probs = {}\n",
    "for tokens, count in vanilla_bigram_freq.items():\n",
    "    vanilla_bigram_probs[tokens] = count / vanilla_unigram_freq[tokens[0]]\n",
    "    \n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Create a vanilla trigram model by creating trigrams from the train_set and counting the frequency of each trigram\n",
    "vanilla_trigram_freq = {}\n",
    "vanilla_total_trigrams = len(create_vanilla_ngrams(train_set, 3))\n",
    "vanilla_trigram_tokens = create_vanilla_ngrams(train_set, 3)\n",
    "for tokens in vanilla_trigram_tokens:\n",
    "    if tokens in vanilla_trigram_freq:\n",
    "        vanilla_trigram_freq[tokens] += 1\n",
    "    else:\n",
    "        vanilla_trigram_freq[tokens] = 1\n",
    "\n",
    "# Calculate vanilla trigram probabilities by dividing the frequency of each trigram by the frequency of its first two words\n",
    "vanilla_trigram_probs = {}\n",
    "for tokens, count in vanilla_trigram_freq.items():\n",
    "    vanilla_trigram_probs[tokens] = count / vanilla_bigram_freq[(tokens[0], tokens[1])]\n",
    "    \n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "## Computation\n",
    "# Convert the dictionaries of the probabilities to a dictionary of string representations\n",
    "vanilla_unigram_probs_str = {str(k): v for k, v in vanilla_unigram_probs.items()}\n",
    "vanilla_bigram_probs_str = {str(k): v for k, v in vanilla_bigram_probs.items()}\n",
    "vanilla_trigram_probs_str = {str(k): v for k, v in vanilla_trigram_probs.items()}\n",
    "\n",
    "# Wrap the string representation of the probabilities in a dictionary\n",
    "vanilla_unigram_probs_dict = {'unigrams': vanilla_unigram_probs_str}\n",
    "vanilla_bigram_probs_dict = {'bigrams': vanilla_bigram_probs_str}\n",
    "vanilla_trigram_probs_dict = {'trigrams': vanilla_trigram_probs_str}\n",
    "\n",
    "# Open a file named 'vanilla_unigram_model.json' and write the dictionary of unigram probabilities to it\n",
    "with open('vanilla_unigram_model.json', 'w') as f:\n",
    "    json.dump(vanilla_unigram_probs_dict, f)\n",
    "\n",
    "# Open a file named 'vanilla_bigram_model.json' and write the dictionary of bigram probabilities to it\n",
    "with open('vanilla_bigram_model.json', 'w') as f:\n",
    "    json.dump(vanilla_bigram_probs_dict, f)\n",
    "\n",
    "# Open a file named 'vanilla_trigram_model.json' and write the dictionary of trigram probabilities to it\n",
    "with open('vanilla_trigram_model.json', 'w') as f:\n",
    "    json.dump(vanilla_trigram_probs_dict, f)\n",
    "\n",
    "# Record the time when the export process ends\n",
    "vanilla_export_end = datetime.now()\n",
    "\n",
    "# Calculate the time taken to export the language model by subtracting the start time from the end time\n",
    "vanilla_export_time = vanilla_export_end - vanilla_export_start\n",
    "\n",
    "# Get the size of the json files\n",
    "vanilla_unigram_file_size = os.path.getsize('vanilla_unigram_model.json')\n",
    "vanilla_bigram_file_size = os.path.getsize('vanilla_bigram_model.json')\n",
    "vanilla_trigram_file_size = os.path.getsize('vanilla_trigram_model.json')\n",
    "\n",
    "# Print the time taken to export the Vanilla model in the format HH:MM:SS:ms\n",
    "print('\\nVanilla Language Model Build & Export Time(HH:MM:SS:ms) - {}\\n'.format(vanilla_export_time))\n",
    "\n",
    "# Print the memory used by the Vanilla model in GB\n",
    "print(\"Vanilla Language Model Memory Use: {:.6f} GB\\n\".format(RAMusage()))\n",
    "\n",
    "# For each file size and name pair, convert the size to a human-readable format and print the size of the file\n",
    "for size, name in [(vanilla_unigram_file_size, 'Vanilla Unigram Model'),\n",
    "                   (vanilla_bigram_file_size, 'Vanilla Bigram Model'),\n",
    "                   (vanilla_trigram_file_size, 'Vanilla Trigram Model')]:\n",
    "    for unit in ['B', 'KB', 'MB', 'GB']:\n",
    "        if size < 1024.0:\n",
    "            size_str = f\"{size:.2f} {unit}\"\n",
    "            break\n",
    "        size /= 1024.0\n",
    "    print(f\"{name} Size: {size_str}\")\n",
    "\n",
    "print()\n",
    "print(\"-\" * 123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8c37b3",
   "metadata": {},
   "source": [
    "### Test output for Vanilla Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2896743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vanilla unigram model (subset):\n",
      "{'the': 169147, 'and': 76286, 'all': 9865, 'to': 76095, 'no': 12954, 'play': 1065, 'i': 48034, 'why': 2479, 'in': 55267, 'about': 7342}\n",
      "\n",
      "Vanilla unigram probabilities (subset):\n",
      "{'the': 0.05281169914625557, 'and': 0.023818295808209735, 'all': 0.0030800866233383455, 'to': 0.02375866108493983, 'no': 0.00404454557716421, 'play': 0.00033251822137408396, 'i': 0.01499735234317629, 'why': 0.0007740025077806142, 'in': 0.017255666235381686, 'about': 0.0022923462735479102}\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Vanilla bigram model (subset):\n",
      "{('the', 'and'): 4052, ('and', 'all'): 236, ('all', 'to'): 247, ('to', 'no'): 313, ('no', 'play'): 3, ('play', 'all'): 4, ('all', 'i'): 158, ('i', 'why'): 42, ('why', 'in'): 40, ('in', 'about'): 126}\n",
      "\n",
      "Vanilla bigram probabilities (subset):\n",
      "{('the', 'and'): 0.023955494333331363, ('and', 'all'): 0.0030936213722045987, ('all', 'to'): 0.025038013177901674, ('to', 'no'): 0.004113279453314935, ('no', 'play'): 0.0002315886984715146, ('play', 'all'): 0.003755868544600939, ('all', 'i'): 0.016016218955904715, ('i', 'why'): 0.0008743806470416788, ('why', 'in'): 0.016135538523598225, ('in', 'about'): 0.0022798414967340366}\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Vanilla trigram model (subset):\n",
      "{('the', 'and', 'all'): 13, ('and', 'all', 'to'): 7, ('all', 'to', 'no'): 1, ('to', 'no', 'play'): 1, ('no', 'play', 'all'): 1, ('play', 'all', 'i'): 1, ('all', 'i', 'why'): 1, ('i', 'why', 'in'): 1, ('why', 'in', 'about'): 1, ('in', 'about', 'have'): 2}\n",
      "\n",
      "Vanilla trigram probabilities (subset):\n",
      "{('the', 'and', 'all'): 0.0032082922013820336, ('and', 'all', 'to'): 0.029661016949152543, ('all', 'to', 'no'): 0.004048582995951417, ('to', 'no', 'play'): 0.003194888178913738, ('no', 'play', 'all'): 0.3333333333333333, ('play', 'all', 'i'): 0.25, ('all', 'i', 'why'): 0.006329113924050633, ('i', 'why', 'in'): 0.023809523809523808, ('why', 'in', 'about'): 0.025, ('in', 'about', 'have'): 0.015873015873015872}\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "## Testing Vanilla Language Model by printing subsets of the model and probability dictionaries\n",
    "\n",
    "# Print the frequency of the first 10 unigrams in the model\n",
    "print(\"\\nVanilla unigram model (subset):\")\n",
    "print({k: vanilla_unigram_freq[k] for k in list(vanilla_unigram_freq)[:10]})\n",
    "\n",
    "# Print the probability of the first 10 unigrams in the model\n",
    "print(\"\\nVanilla unigram probabilities (subset):\")\n",
    "print({k: vanilla_unigram_probs[k] for k in list(vanilla_unigram_probs)[:10]})\n",
    "print(\"-\"*127)\n",
    "\n",
    "# Print the frequency of the first 10 bigrams in the model\n",
    "print(\"\\nVanilla bigram model (subset):\")\n",
    "print({k: vanilla_bigram_freq[k] for k in list(vanilla_bigram_freq)[:10]})\n",
    "\n",
    "# Print the probability of the first 10 bigrams in the model\n",
    "print(\"\\nVanilla bigram probabilities (subset):\")\n",
    "print({k: vanilla_bigram_probs[k] for k in list(vanilla_bigram_probs)[:10]})\n",
    "print(\"-\"*127)\n",
    "\n",
    "# Print the frequency of the first 10 trigrams in the model\n",
    "print(\"\\nVanilla trigram model (subset):\")\n",
    "print({k: vanilla_trigram_freq[k] for k in list(vanilla_trigram_freq)[:10]})\n",
    "\n",
    "# Print the probability of the first 10 trigrams in the model\n",
    "print(\"\\nVanilla trigram probabilities (subset):\")\n",
    "print({k: vanilla_trigram_probs[k] for k in list(vanilla_trigram_probs)[:10]})\n",
    "print(\"-\"*127)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a99719",
   "metadata": {},
   "source": [
    "## Laplace Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cab7ff62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Laplace Language Model Build & Export Time(HH:MM:SS:ms) - 0:00:44.627423\n",
      "\n",
      "Laplace Language Model Memory Use: 2.355812 GB\n",
      "\n",
      "Laplace Unigram Model Size: 2.20 MB\n",
      "LaplaceBigram Model Size: 61.61 MB\n",
      "Laplace Trigram Model Size: 144.52 MB\n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Record start time for model export\n",
    "laplace_export_start = datetime.now()\n",
    "\n",
    "# Define function to create n-grams from a list of tokens\n",
    "def create_laplace_ngrams(tokens, n):\n",
    "    return [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Create a Laplace unigram model by iterating over the train_set and counting the frequency of each word\n",
    "laplace_unigram_freq = {}\n",
    "laplace_total_unigrams = len(train_set)\n",
    "for word in train_set:\n",
    "    if word in laplace_unigram_freq:\n",
    "        laplace_unigram_freq[word] += 1\n",
    "    else:\n",
    "        laplace_unigram_freq[word] = 1\n",
    "\n",
    "# Calculate Laplace unigram probabilities by adding 1 to the frequency of each word and dividing by the total number of unigrams plus the vocabulary size\n",
    "laplace_unigram_probs = {}\n",
    "for word, count in laplace_unigram_freq.items():\n",
    "    laplace_unigram_probs[word] = (count + 1) / (laplace_total_unigrams + len(laplace_unigram_freq))\n",
    "    \n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "# Create a Laplace bigram model by creating bigrams from the train_set and counting the frequency of each bigram\n",
    "laplace_bigram_freq = {}\n",
    "laplace_total_bigrams = len(create_laplace_ngrams(train_set, 2))\n",
    "laplace_bigram_tokens = create_laplace_ngrams(train_set, 2)\n",
    "for tokens in laplace_bigram_tokens:\n",
    "    if tokens in laplace_bigram_freq:\n",
    "        laplace_bigram_freq[tokens] += 1\n",
    "    else:\n",
    "        laplace_bigram_freq[tokens] = 1\n",
    "\n",
    "# Calculate Laplace bigram probabilities by adding 1 to the frequency of each bigram and dividing by the frequency of its first word plus the vocabulary size\n",
    "laplace_bigram_probs = {}\n",
    "for tokens, count in laplace_bigram_freq.items():\n",
    "    laplace_bigram_probs[tokens] = (count + 1) / (laplace_unigram_freq[tokens[0]] + len(laplace_unigram_freq))\n",
    "    \n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "# Create a Laplace trigram model by creating trigrams from the train_set and counting the frequency of each trigram\n",
    "laplace_trigram_freq = {}\n",
    "laplace_total_trigrams = len(create_laplace_ngrams(train_set, 3))\n",
    "laplace_trigram_tokens = create_laplace_ngrams(train_set, 3)\n",
    "for tokens in laplace_trigram_tokens:\n",
    "    if tokens in laplace_trigram_freq:\n",
    "        laplace_trigram_freq[tokens] += 1\n",
    "    else:\n",
    "        laplace_trigram_freq[tokens] = 1\n",
    "\n",
    "# Calculate Laplace trigram probabilities by adding 1 to the frequency of each trigram and dividing by the frequency of its first two words plus the vocabulary size\n",
    "laplace_trigram_probs = {}\n",
    "for tokens, count in laplace_trigram_freq.items():\n",
    "    laplace_trigram_probs[tokens] = (count + 1) / (laplace_bigram_freq[(tokens[0], tokens[1])] + len(laplace_unigram_freq))\n",
    "    \n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "## Computation\n",
    "# Convert the dictionaries of the probabilities to a dictionary of string representations\n",
    "laplace_unigram_probs_str = {str(k): v for k, v in laplace_unigram_probs.items()}\n",
    "laplace_bigram_probs_str = {str(k): v for k, v in laplace_bigram_probs.items()}\n",
    "laplace_trigram_probs_str = {str(k): v for k, v in laplace_trigram_probs.items()}\n",
    "\n",
    "# Wrap the string representation of the probabilities in a dictionary\n",
    "laplace_unigram_probs_dict = {'unigrams': laplace_unigram_probs_str}\n",
    "laplace_bigram_probs_dict = {'bigrams': laplace_bigram_probs_str}\n",
    "laplace_trigram_probs_dict = {'trigrams': laplace_trigram_probs_str}\n",
    "\n",
    "# Open a file named 'laplace_unigram_model.json' and write the dictionary of unigram probabilities to it\n",
    "with open('laplace_unigram_model.json', 'w') as f:\n",
    "    json.dump(laplace_unigram_probs_dict, f)\n",
    "\n",
    "# Open a file named 'laplace_bigram_model.json' and write the dictionary of bigram probabilities to it\n",
    "with open('laplace_bigram_model.json', 'w') as f:\n",
    "    json.dump(laplace_bigram_probs_dict, f)\n",
    "\n",
    "# Open a file named 'laplace_trigram_model.json' and write the dictionary of trigram probabilities to it\n",
    "with open('laplace_trigram_model.json', 'w') as f:\n",
    "    json.dump(laplace_trigram_probs_dict, f)\n",
    "\n",
    "# Record the time when the export process ends\n",
    "laplace_export_end = datetime.now()\n",
    "\n",
    "# Calculate the time taken to export the language model by subtracting the start time from the end time\n",
    "laplace_export_time = laplace_export_end - laplace_export_start\n",
    "\n",
    "# Get the size of the json files\n",
    "laplace_unigram_file_size = os.path.getsize('laplace_unigram_model.json')\n",
    "laplace_bigram_file_size = os.path.getsize('laplace_bigram_model.json')\n",
    "laplace_trigram_file_size = os.path.getsize('laplace_trigram_model.json')\n",
    "\n",
    "# Print the time taken to export the Laplace model in the format HH:MM:SS:ms\n",
    "print('\\nLaplace Language Model Build & Export Time(HH:MM:SS:ms) - {}\\n'.format(laplace_export_time))\n",
    "\n",
    "# Print the memory used by the Laplace model in GB\n",
    "print(\"Laplace Language Model Memory Use: {:.6f} GB\\n\".format(RAMusage()))\n",
    "\n",
    "# For each file size and name pair, convert the size to a human-readable format and print the size of the file\n",
    "for size, name in [(laplace_unigram_file_size, 'Laplace Unigram Model'),\n",
    "                   (laplace_bigram_file_size, 'LaplaceBigram Model'),\n",
    "                   (laplace_trigram_file_size, 'Laplace Trigram Model')]:\n",
    "    for unit in ['B', 'KB', 'MB', 'GB']:\n",
    "        if size < 1024.0:\n",
    "            size_str = f\"{size:.2f} {unit}\"\n",
    "            break\n",
    "        size /= 1024.0\n",
    "    print(f\"{name} Size: {size_str}\")\n",
    "\n",
    "print()\n",
    "print(\"-\" * 123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb21e853",
   "metadata": {},
   "source": [
    "### Test output for Laplace  Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f32d1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Laplace unigram model (subset):\n",
      "{'the': 169147, 'and': 76286, 'all': 9865, 'to': 76095, 'no': 12954, 'play': 1065, 'i': 48034, 'why': 2479, 'in': 55267, 'about': 7342}\n",
      "\n",
      "Laplace unigram probabilities (subset):\n",
      "{'the': 0.05175974503854869, 'and': 0.023344028127768368, 'all': 0.00301902265797007, 'to': 0.023285581611685632, 'no': 0.0039642650044599895, 'play': 0.00032619888033611334, 'i': 0.014698839790755352, 'why': 0.0007588867009695696, 'in': 0.01691215733434926, 'about': 0.0022469778408143342}\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Laplace bigram model (subset):\n",
      "{('the', 'and'): 4052, ('and', 'all'): 236, ('all', 'to'): 247, ('to', 'no'): 313, ('no', 'play'): 3, ('play', 'all'): 4, ('all', 'i'): 158, ('i', 'why'): 42, ('why', 'in'): 40, ('in', 'about'): 126}\n",
      "\n",
      "Laplace bigram probabilities (subset):\n",
      "{('the', 'and'): 0.01730128916588406, ('and', 'all'): 0.001676108034710288, ('all', 'to'): 0.0033076369068260024, ('to', 'no'): 0.0022236700470228314, ('no', 'play'): 5.1238039120242866e-05, ('play', 'all'): 7.555380942307112e-05, ('all', 'i'): 0.0021206220491344126, ('i', 'why'): 0.0003800365895693213, ('why', 'in'): 0.0006065806604331873, ('in', 'about'): 0.0010549925236750292}\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Laplace trigram model (subset):\n",
      "{('the', 'and', 'all'): 13, ('and', 'all', 'to'): 7, ('all', 'to', 'no'): 1, ('to', 'no', 'play'): 1, ('no', 'play', 'all'): 1, ('play', 'all', 'i'): 1, ('all', 'i', 'why'): 1, ('i', 'why', 'in'): 1, ('why', 'in', 'about'): 1, ('in', 'about', 'have'): 2}\n",
      "\n",
      "Laplace trigram probabilities (subset):\n",
      "{('the', 'and', 'all'): 0.00020241451601243403, ('and', 'all', 'to'): 0.00012241962386570567, ('all', 'to', 'no'): 3.0599755201958386e-05, ('to', 'no', 'play'): 3.056888698682481e-05, ('no', 'play', 'all'): 3.0714417347502915e-05, ('play', 'all', 'i'): 3.071394566703011e-05, ('all', 'i', 'why'): 3.064147937062401e-05, ('i', 'why', 'in'): 3.069603253779449e-05, ('why', 'in', 'about'): 3.0696974813132164e-05, ('in', 'about', 'have'): 4.598476371495578e-05}\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "## Testing Laplace Language Model by printing subsets of the model and probability dictionaries\n",
    "\n",
    "# Print the frequency of the first 10 unigrams in the model\n",
    "print(\"\\nLaplace unigram model (subset):\")\n",
    "print({k: laplace_unigram_freq[k] for k in list(laplace_unigram_freq)[:10]})\n",
    "\n",
    "# Print the probability of the first 10 unigrams in the model\n",
    "print(\"\\nLaplace unigram probabilities (subset):\")\n",
    "print({k: laplace_unigram_probs[k] for k in list(laplace_unigram_probs)[:10]})\n",
    "print(\"-\"*127)\n",
    "\n",
    "# Print the frequency of the first 10 bigrams in the model\n",
    "print(\"\\nLaplace bigram model (subset):\")\n",
    "print({k: laplace_bigram_freq[k] for k in list(laplace_bigram_freq)[:10]})\n",
    "\n",
    "# Print the probability of the first 10 bigrams in the model\n",
    "print(\"\\nLaplace bigram probabilities (subset):\")\n",
    "print({k: laplace_bigram_probs[k] for k in list(laplace_bigram_probs)[:10]})\n",
    "print(\"-\"*127)\n",
    "\n",
    "# Print the frequency of the first 10 trigrams in the model\n",
    "print(\"\\nLaplace trigram model (subset):\")\n",
    "print({k: laplace_trigram_freq[k] for k in list(laplace_trigram_freq)[:10]})\n",
    "\n",
    "# Print the probability of the first 10 trigrams in the model\n",
    "print(\"\\nLaplace trigram probabilities (subset):\")\n",
    "print({k: laplace_trigram_probs[k] for k in list(laplace_trigram_probs)[:10]}) \n",
    "print(\"-\"*127)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11b3ff7",
   "metadata": {},
   "source": [
    "## UNK Language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c4c2c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "UNK Language Model Build & Export Time(HH:MM:SS:ms) - 0:00:51.174329\n",
      "\n",
      "UNK Language Model Memory Use: 2.995838 GB\n",
      "\n",
      "UNK Unigram Model Size: 986.81 KB\n",
      "UNK Bigram Model Size: 53.46 MB\n",
      "UNK Trigram Model Size: 138.20 MB\n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Record start time for model export\n",
    "unk_export_start = datetime.now()\n",
    "\n",
    "# Create a new list with <UNK> tokens for words with count less than or equal to 2\n",
    "unk_train_set = []\n",
    "word_freq = {}\n",
    "for word in train_set:\n",
    "    if word in word_freq:\n",
    "        word_freq[word] += 1\n",
    "    else:\n",
    "        word_freq[word] = 1\n",
    "    if word_freq[word] <= 2:\n",
    "        unk_train_set.append('<UNK>')\n",
    "    else:\n",
    "        unk_train_set.append(word)\n",
    "\n",
    "# Create UNK Laplace unigram model\n",
    "unk_laplace_unigram_freq = {}\n",
    "unk_laplace_total_unigrams = len(unk_train_set)\n",
    "for word in unk_train_set:\n",
    "    if word in unk_laplace_unigram_freq:\n",
    "        unk_laplace_unigram_freq[word] += 1\n",
    "    else:\n",
    "        unk_laplace_unigram_freq[word] = 1\n",
    "\n",
    "# Calculate UNK Laplace unigram probabilities\n",
    "unk_laplace_unigram_probs = {}\n",
    "for word, count in unk_laplace_unigram_freq.items():\n",
    "    unk_laplace_unigram_probs[word] = (count + 1) / (unk_laplace_total_unigrams + len(unk_laplace_unigram_freq))\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Create UNK Laplace bigram model\n",
    "unk_laplace_bigram_freq = {}\n",
    "unk_laplace_total_bigrams = len(create_laplace_ngrams(unk_train_set, 2))\n",
    "unk_laplace_bigram_tokens = create_laplace_ngrams(unk_train_set, 2)\n",
    "for tokens in unk_laplace_bigram_tokens:\n",
    "    if tokens in unk_laplace_bigram_freq:\n",
    "        unk_laplace_bigram_freq[tokens] += 1\n",
    "    else:\n",
    "        unk_laplace_bigram_freq[tokens] = 1\n",
    "\n",
    "# Calculate UNK Laplace bigram probabilities\n",
    "unk_laplace_bigram_probs = {}\n",
    "for tokens, count in unk_laplace_bigram_freq.items():\n",
    "    unk_laplace_bigram_probs[tokens] = (count + 1) / (unk_laplace_unigram_freq[tokens[0]] + len(unk_laplace_unigram_freq))\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Create UNK Laplace trigram model\n",
    "unk_laplace_trigram_freq = {}\n",
    "unk_laplace_total_trigrams = len(create_laplace_ngrams(unk_train_set, 3))\n",
    "unk_laplace_trigram_tokens = create_laplace_ngrams(unk_train_set, 3)\n",
    "for tokens in unk_laplace_trigram_tokens:\n",
    "    if tokens in unk_laplace_trigram_freq:\n",
    "        unk_laplace_trigram_freq[tokens] += 1\n",
    "    else:\n",
    "        unk_laplace_trigram_freq[tokens] = 1\n",
    "\n",
    "# Calculate UNK Laplace trigram probabilities\n",
    "unk_laplace_trigram_probs = {}\n",
    "for tokens, count in unk_laplace_trigram_freq.items():\n",
    "    unk_laplace_trigram_probs[tokens] = (count + 1) / (unk_laplace_bigram_freq[(tokens[0], tokens[1])] + len(unk_laplace_unigram_freq))\n",
    "    \n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "## Computation\n",
    "# Convert the dictionaries of the probabilities to a dictionary of string representations\n",
    "unk_laplace_unigram_probs_str  = {str(k): v for k, v in unk_laplace_unigram_probs.items()}\n",
    "unk_laplace_bigram_probs_str  = {str(k): v for k, v in unk_laplace_bigram_probs.items()}\n",
    "unk_laplace_trigram_probs_str = {str(k): v for k, v in  unk_laplace_trigram_probs.items()}\n",
    "\n",
    "# Wrap the string representation of the probabilities in a dictionary\n",
    "unk_unigram_probs_dict = {'unigrams': unk_laplace_unigram_probs_str}\n",
    "unk_bigram_probs_dict = {'bigrams':   unk_laplace_bigram_probs_str}\n",
    "unk_trigram_probs_dict = {'trigrams': unk_laplace_trigram_probs_str}\n",
    "\n",
    "# Open a file named 'unk_unigram_model.json' and write the dictionary of unigram probabilities to it\n",
    "with open('unk_unigram_model.json', 'w') as f:\n",
    "    json.dump(unk_unigram_probs_dict, f)\n",
    "\n",
    "# Open a file named 'unk_bigram_model.json' and write the dictionary of bigram probabilities to it\n",
    "with open('unk_bigram_model.json', 'w') as f:\n",
    "    json.dump(unk_bigram_probs_dict, f)\n",
    "\n",
    "# Open a file named 'unk_trigram_model.json' and write the dictionary of trigram probabilities to it\n",
    "with open('unk_trigram_model.json', 'w') as f:\n",
    "    json.dump(unk_trigram_probs_dict, f)\n",
    "\n",
    "# Record the time when the export process ends\n",
    "unk_export_end = datetime.now()\n",
    "\n",
    "# Calculate the time taken to export the language model by subtracting the start time from the end time\n",
    "unk_export_time = unk_export_end - unk_export_start\n",
    "\n",
    "# Get the size of the json files\n",
    "unk_unigram_file_size = os.path.getsize('unk_unigram_model.json')\n",
    "unk_bigram_file_size = os.path.getsize('unk_bigram_model.json')\n",
    "unk_trigram_file_size = os.path.getsize('unk_trigram_model.json')\n",
    "\n",
    "# Print the time taken to export the UNK model in the format HH:MM:SS:ms\n",
    "print('\\nUNK Language Model Build & Export Time(HH:MM:SS:ms) - {}\\n'.format(unk_export_time))\n",
    "\n",
    "# Print the memory used by the UNK model in GB\n",
    "print(\"UNK Language Model Memory Use: {:.6f} GB\\n\".format(RAMusage()))\n",
    "\n",
    "# For each file size and name pair, convert the size to a human-readable format and print the size of the file\n",
    "for size, name in [(unk_unigram_file_size, 'UNK Unigram Model'),\n",
    "                   (unk_bigram_file_size, 'UNK Bigram Model'),\n",
    "                   (unk_trigram_file_size, 'UNK Trigram Model')]:\n",
    "    for unit in ['B', 'KB', 'MB', 'GB']:\n",
    "        if size < 1024.0:\n",
    "            size_str = f\"{size:.2f} {unit}\"\n",
    "            break\n",
    "        size /= 1024.0\n",
    "    print(f\"{name} Size: {size_str}\")\n",
    "\n",
    "print()\n",
    "print(\"-\" * 123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445c5697",
   "metadata": {},
   "source": [
    "### Test output for UNK Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68ca8b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "UNK Laplace unigram model (subset):\n",
      "{'<UNK>': 102853, 'to': 76093, 'and': 76284, 'the': 169145, 'is': 32533, 'all': 9863, 's': 38857, 'a': 90407, 'i': 48032, 'in': 55265}\n",
      "\n",
      "UNK Laplace unigram probabilities (subset):\n",
      "{'<UNK>': 0.03182428849103973, 'to': 0.023544416439197087, 'and': 0.023603514180673243, 'the': 0.05233584596715156, 'is': 0.010066418435524983, 'all': 0.003052042523145584, 's': 0.012023141561677931, 'a': 0.027973343515059403, 'i': 0.014861999038346698, 'in': 0.017099977907964706}\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "UNK Laplace bigram model (subset):\n",
      "{('<UNK>', '<UNK>'): 6914, ('<UNK>', 'to'): 2426, ('to', '<UNK>'): 2438, ('<UNK>', 'and'): 2443, ('and', '<UNK>'): 2516, ('<UNK>', 'the'): 5460, ('the', '<UNK>'): 5359, ('<UNK>', 'is'): 1025, ('is', '<UNK>'): 1068, ('<UNK>', 'all'): 334}\n",
      "\n",
      "UNK Laplace bigram probabilities (subset):\n",
      "{('<UNK>', '<UNK>'): 0.05240422871433443, ('<UNK>', 'to'): 0.0183926338524497, ('to', '<UNK>'): 0.023185512619421075, ('<UNK>', 'and'): 0.01852146565116896, ('and', '<UNK>'): 0.02388362780634999, ('<UNK>', 'the'): 0.0413853207532871, ('the', '<UNK>'): 0.027036979122004368, ('<UNK>', 'is'): 0.007775377969762419, ('is', '<UNK>'): 0.017344041534842217, ('<UNK>', 'all'): 0.0025387442688795422}\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "UNK Laplace trigram model (subset):\n",
      "{('<UNK>', '<UNK>', '<UNK>'): 1290, ('<UNK>', '<UNK>', 'to'): 170, ('<UNK>', 'to', '<UNK>'): 164, ('to', '<UNK>', '<UNK>'): 164, ('<UNK>', '<UNK>', 'and'): 171, ('<UNK>', 'and', '<UNK>'): 183, ('and', '<UNK>', '<UNK>'): 174, ('<UNK>', '<UNK>', 'the'): 360, ('<UNK>', 'the', '<UNK>'): 376, ('the', '<UNK>', '<UNK>'): 362}\n",
      "\n",
      "UNK Laplace trigram probabilities (subset):\n",
      "{('<UNK>', '<UNK>', '<UNK>'): 0.03584517992003554, ('<UNK>', '<UNK>', 'to'): 0.004747889826743669, ('<UNK>', 'to', '<UNK>'): 0.005233443288505456, ('to', '<UNK>', '<UNK>'): 0.00523145212428662, ('<UNK>', '<UNK>', 'and'): 0.004775655264326966, ('<UNK>', 'and', '<UNK>'): 0.0058329370740212395, ('and', '<UNK>', '<UNK>'): 0.005534821936871402, ('<UNK>', '<UNK>', 'the'): 0.01002332296756997, ('<UNK>', 'the', '<UNK>'): 0.01090793356865922, ('the', '<UNK>', '<UNK>'): 0.010533646731087316}\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "## Testing UNK Language Model by printing subsets of the model and probability dictionaries\n",
    "\n",
    "# Print the frequency of the first 10 unigrams in the model\n",
    "print(\"\\nUNK Laplace unigram model (subset):\")\n",
    "print({k: unk_laplace_unigram_freq[k] for k in list(unk_laplace_unigram_freq)[:10]})\n",
    "\n",
    "# Print the probability of the first 10 unigrams in the model\n",
    "print(\"\\nUNK Laplace unigram probabilities (subset):\")\n",
    "print({k: unk_laplace_unigram_probs[k] for k in list(unk_laplace_unigram_probs)[:10]})\n",
    "print(\"-\"*127)\n",
    "\n",
    "# Print the frequency of the first 10 bigrams in the model\n",
    "print(\"\\nUNK Laplace bigram model (subset):\")\n",
    "print({k: unk_laplace_bigram_freq[k] for k in list(unk_laplace_bigram_freq)[:10]})\n",
    "\n",
    "# Print the probability of the first 10 bigrams in the model\n",
    "print(\"\\nUNK Laplace bigram probabilities (subset):\")\n",
    "print({k: unk_laplace_bigram_probs[k] for k in list(unk_laplace_bigram_probs)[:10]})\n",
    "print(\"-\"*127)\n",
    "\n",
    "# Print the frequency of the first 10 trigrams in the model\n",
    "print(\"\\nUNK Laplace trigram model (subset):\")\n",
    "print({k: unk_laplace_trigram_freq[k] for k in list(unk_laplace_trigram_freq)[:10]})\n",
    "\n",
    "# Print the probability of the first 10 trigrams in the model\n",
    "print(\"\\nUNK Laplace trigram probabilities (subset):\")\n",
    "print({k: unk_laplace_trigram_probs[k] for k in list(unk_laplace_trigram_probs)[:10]})\n",
    "print(\"-\"*127)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed85418",
   "metadata": {},
   "source": [
    "## Linear Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1395f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vanilla LM: Probability of 'this is a test sentence': 0.028493\n",
      "Laplace LM: Probability of 'this is a test sentence': 0.023182\n",
      "UNK LM: Probability of 'this is a test sentence': 0.026509\n"
     ]
    }
   ],
   "source": [
    "def linear_interpolation(sentence, lm_type):\n",
    "    # Set the lambda values for each n-gram model\n",
    "    lambda_3 = 0.6\n",
    "    lambda_2 = 0.3\n",
    "    lambda_1 = 0.1\n",
    "    \n",
    "    # Tokenize the sentence\n",
    "    sentence_tokens = sentence.split()\n",
    "    \n",
    "    # Calculate the probabilities of each n-gram model for the sentence\n",
    "    if lm_type == \"vanilla\":\n",
    "        unigram_probs = [vanilla_unigram_probs[token] for token in sentence_tokens]\n",
    "        bigram_probs = [vanilla_bigram_probs.get((sentence_tokens[i-1], sentence_tokens[i]), 0) for i in range(1, len(sentence_tokens))]\n",
    "        trigram_probs = [vanilla_trigram_probs.get((sentence_tokens[i-2], sentence_tokens[i-1], sentence_tokens[i]), 0) for i in range(2, len(sentence_tokens))]\n",
    "    elif lm_type == \"laplace\":\n",
    "        unigram_probs = [(laplace_unigram_freq.get(token, 0) + 1) / (laplace_total_unigrams + len(laplace_unigram_freq)) for token in sentence_tokens]\n",
    "        bigram_probs = [(laplace_bigram_freq.get((sentence_tokens[i-1], sentence_tokens[i]), 0) + 1) / (laplace_unigram_freq.get(sentence_tokens[i-1], 0) + len(laplace_unigram_freq)) for i in range(1, len(sentence_tokens))]\n",
    "        trigram_probs = [(laplace_trigram_freq.get((sentence_tokens[i-2], sentence_tokens[i-1], sentence_tokens[i]), 0) + 1) / (laplace_bigram_freq.get((sentence_tokens[i-2], sentence_tokens[i-1]), 0) + len(laplace_unigram_freq)) for i in range(2, len(sentence_tokens))]\n",
    "    elif lm_type == \"unk\":\n",
    "        # Replace infrequent words with <UNK>\n",
    "        unk_sentence_tokens = ['<UNK>' if word_freq[token] <= 2 else token for token in sentence_tokens]\n",
    "        # Calculate the probabilities for the UNK Laplace model\n",
    "        unk_laplace_unigram_probs = {word: (count + 1) / (unk_laplace_total_unigrams + len(unk_laplace_unigram_freq)) for word, count in unk_laplace_unigram_freq.items()}\n",
    "        unk_laplace_bigram_probs = {(word1, word2): (count + 1) / (unk_laplace_unigram_freq.get(word1, 0) + len(unk_laplace_unigram_freq)) for (word1, word2), count in unk_laplace_bigram_freq.items()}\n",
    "        unk_laplace_trigram_probs = {(word1, word2, word3): (count + 1) / (unk_laplace_bigram_freq.get((word1, word2), 0) + len(unk_laplace_unigram_freq)) for (word1, word2, word3), count in unk_laplace_trigram_freq.items()}\n",
    "        unk_laplace_trigram_probs = {(word1, word2, word3): (count + 1) / (unk_laplace_bigram_freq.get((word1, word2), 0) + len(unk_laplace_unigram_freq)) for (word1, word2, word3), count in unk_laplace_trigram_freq.items()}\n",
    "        # Replace tokens in sentence with <UNK> where appropriate\n",
    "        unigram_probs = [unk_laplace_unigram_probs.get(token, 1 / (unk_laplace_total_unigrams + len(unk_laplace_unigram_freq))) for token in unk_sentence_tokens]\n",
    "        bigram_probs = [(unk_laplace_bigram_freq.get((unk_sentence_tokens[i-1], unk_sentence_tokens[i]), 0) + 1) / (unk_laplace_unigram_freq.get(unk_sentence_tokens[i-1], 0) + len(unk_laplace_unigram_freq)) for i in range(1, len(unk_sentence_tokens))]\n",
    "        trigram_probs = [(unk_laplace_trigram_probs.get((unk_sentence_tokens[i-2], unk_sentence_tokens[i-1], unk_sentence_tokens[i]), 0) + 1) / (unk_laplace_bigram_freq.get((unk_sentence_tokens[i-2], unk_sentence_tokens[i-1]), 0) + len(unk_laplace_unigram_freq)) for i in range(2, len(unk_sentence_tokens))]\n",
    "        \n",
    "    # Calculate the probability of the sentence using linear interpolation\n",
    "    sentence_prob = 1\n",
    "    for i in range(len(sentence_tokens)):\n",
    "        if i == 0:\n",
    "            sentence_prob *= unigram_probs[i]**lambda_1\n",
    "        elif i == 1:\n",
    "            sentence_prob *= (lambda_2*bigram_probs[i-1] + lambda_1*unigram_probs[i])**lambda_1\n",
    "        else:\n",
    "            sentence_prob *= (lambda_3*trigram_probs[i-2] + lambda_2*bigram_probs[i-1] + lambda_1*unigram_probs[i])**lambda_1\n",
    "\n",
    "    return sentence_prob\n",
    "\n",
    "# Define some test sentences\n",
    "sentences = [\"this is a test sentence\"]\n",
    "\n",
    "# Test the vanilla language model\n",
    "for sentence in sentences:\n",
    "    prob = linear_interpolation(sentence, \"vanilla\")\n",
    "    print(f\"\\nVanilla LM: Probability of '{sentence}': {prob:.6f}\")\n",
    "\n",
    "# Test the laplace language model\n",
    "for sentence in sentences:\n",
    "    prob = linear_interpolation(sentence, \"laplace\")\n",
    "    print(f\"Laplace LM: Probability of '{sentence}': {prob:.6f}\")\n",
    "\n",
    "# Test the unk language model\n",
    "for sentence in sentences:\n",
    "    prob = linear_interpolation(sentence, \"unk\")\n",
    "    print(f\"UNK LM: Probability of '{sentence}': {prob:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f2313b",
   "metadata": {},
   "source": [
    "## Evaluation & Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aedd4a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model           Unigram         Bigram          Trigram        \n",
      "------------------------------------------------------------\n",
      "Vanilla         120.18          13.59           1.53           \n",
      "Laplace         120.71          36.11           2.55           \n",
      "UNK             110.25          27.61           2.36           \n"
     ]
    }
   ],
   "source": [
    "# This function calculates the perplexity of a given test set using the specified n-gram language model probabilities\n",
    "def calculate_perplexity(test_set, lm_probs, ngram_type):\n",
    "    # Get the length of the test set\n",
    "    N = len(test_set)\n",
    "    # Initialize the sum of the log probabilities to 0\n",
    "    log_prob_sum = 0\n",
    "\n",
    "    # Iterate over the test set and calculate the log probability of each n-gram in the test set\n",
    "    for i in range(len(test_set)):\n",
    "        if ngram_type == \"unigram\":\n",
    "            # Calculate the log probability of the unigram\n",
    "            log_prob_sum += math.log(lm_probs.get(test_set[i], 1))\n",
    "        elif ngram_type == \"bigram\" and i > 0:\n",
    "            # Calculate the log probability of the bigram\n",
    "            log_prob_sum += math.log(lm_probs.get((test_set[i-1], test_set[i]), 1))\n",
    "        elif ngram_type == \"trigram\" and i > 1:\n",
    "            # Calculate the log probability of the trigram\n",
    "            log_prob_sum += math.log(lm_probs.get((test_set[i-2], test_set[i-1], test_set[i]), 1))\n",
    "\n",
    "    # Calculate the entropy and perplexity of the test set using the log probabilities\n",
    "    entropy = -(log_prob_sum / N)\n",
    "    perplexity = math.pow(2, entropy)\n",
    "\n",
    "    # Return the perplexity\n",
    "    return perplexity\n",
    "\n",
    "# Calculate perplexities for each model and n-gram\n",
    "vanilla_unigram_perplexity = calculate_perplexity(test_set, vanilla_unigram_probs, \"unigram\")\n",
    "vanilla_bigram_perplexity = calculate_perplexity(test_set, vanilla_bigram_probs, \"bigram\")\n",
    "vanilla_trigram_perplexity = calculate_perplexity(test_set, vanilla_trigram_probs, \"trigram\")\n",
    "\n",
    "laplace_unigram_perplexity = calculate_perplexity(test_set, laplace_unigram_probs, \"unigram\")\n",
    "laplace_bigram_perplexity = calculate_perplexity(test_set, laplace_bigram_probs, \"bigram\")\n",
    "laplace_trigram_perplexity = calculate_perplexity(test_set, laplace_trigram_probs, \"trigram\")\n",
    "\n",
    "unk_unigram_perplexity = calculate_perplexity(test_set, unk_laplace_unigram_probs, \"unigram\")\n",
    "unk_bigram_perplexity = calculate_perplexity(test_set, unk_laplace_bigram_probs, \"bigram\")\n",
    "unk_trigram_perplexity = calculate_perplexity(test_set, unk_laplace_trigram_probs, \"trigram\")\n",
    "\n",
    "# Print the results in a table format\n",
    "print(\"\\n{:<15} {:<15} {:<15} {:<15}\".format(\"Model\", \"Unigram\", \"Bigram\", \"Trigram\"))\n",
    "print(\"-\" * 60)\n",
    "print(\"{:<15} {:<15.2f} {:<15.2f} {:<15.2f}\".format(\"Vanilla\", vanilla_unigram_perplexity, vanilla_bigram_perplexity, vanilla_trigram_perplexity))\n",
    "print(\"{:<15} {:<15.2f} {:<15.2f} {:<15.2f}\".format(\"Laplace\", laplace_unigram_perplexity, laplace_bigram_perplexity, laplace_trigram_perplexity))\n",
    "print(\"{:<15} {:<15.2f} {:<15.2f} {:<15.2f}\".format(\"UNK\", unk_unigram_perplexity, unk_bigram_perplexity, unk_trigram_perplexity))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a94695",
   "metadata": {},
   "source": [
    "## Genetarion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83f6d118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Which language model would you like to use? (Vanilla, Laplace, UNK): UNK\n",
      "Please enter a phrase: lets go to the\n",
      "\n",
      "Generating unk model...\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Unigram: lets go to the discovery medium owner undergrowth in distance finished it marie i <UNK> khan had coat day\n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Bigram: lets go to the if guessing make your i the monaco god were u the me suspect gironella protect\n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Trigram: lets go to the up definition and go a cokey more are others so mark that his voting seven\n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Define a function to generate a sentence using a language model and a given phrase\n",
    "def generate_sentence(model, phrase):\n",
    "    # Print a message indicating which model is being used\n",
    "    print(f\"\\nGenerating {model} model...\")\n",
    "    print(\"-\" * 127)\n",
    "    # Loop through each n-gram type and generate a sentence for each type\n",
    "    for ngram_type in [\"unigram\", \"bigram\", \"trigram\"]:\n",
    "        # Generate a sentence using the selected model, phrase, and n-gram type\n",
    "        generated_sentence = generate_ngram_sentence(model, phrase, ngram_type)\n",
    "        # Print the generated sentence along with its n-gram type\n",
    "        print(f\"\\n{ngram_type.capitalize()}: {generated_sentence}\\n\")\n",
    "        print(\"-\" * 127)\n",
    "\n",
    "# Define a function to generate a sentence using a specific n-gram type and language model\n",
    "def generate_ngram_sentence(model, phrase, ngram_type):\n",
    "    # Define start and end tokens for the sentence\n",
    "    start_token = '<s>'\n",
    "    end_token = '</s>'\n",
    "    # Set a maximum length for the sentence\n",
    "    max_length = 20\n",
    "    # Initialize the sentence with the start token and the words in the input phrase\n",
    "    sentence = [start_token] + phrase.split()\n",
    "    # Initialize the model probabilities to None\n",
    "    model_probs = None\n",
    "    \n",
    "    # Convert the model name to lowercase for easier comparison\n",
    "    model = model.lower()\n",
    "    \n",
    "    # Determine which set of probabilities to use based on the selected model\n",
    "    if model == 'vanilla':\n",
    "        model_probs = [vanilla_unigram_probs, vanilla_bigram_probs, vanilla_trigram_probs]\n",
    "    elif model == 'laplace':\n",
    "        model_probs = [laplace_unigram_probs, laplace_bigram_probs, laplace_trigram_probs]\n",
    "    elif model == 'unk':\n",
    "        model_probs = [unk_laplace_unigram_probs, unk_laplace_bigram_probs, unk_laplace_trigram_probs]\n",
    "    else:\n",
    "        # If an invalid model was selected, print an error message and return None\n",
    "        print('Invalid model selection')\n",
    "        return\n",
    "\n",
    "    # Generate the sentence by adding words one at a time until the end token is reached or the maximum length is reached\n",
    "    while sentence[-1] != end_token and len(sentence) < max_length:\n",
    "        # Determine the next word to add based on the selected n-gram type\n",
    "        if ngram_type == \"unigram\":\n",
    "            next_word = generate_word(model_probs[0])\n",
    "        elif ngram_type == \"bigram\":\n",
    "            next_word = generate_word(model_probs[1], sentence[-1:])\n",
    "        elif ngram_type == \"trigram\":\n",
    "            if len(sentence) < 2:\n",
    "                next_word = generate_word(model_probs[1], sentence[-1:])\n",
    "            else:\n",
    "                next_word = generate_word(model_probs[2], sentence[-2:])\n",
    "        # Add the next word to the sentence\n",
    "        sentence.append(next_word)\n",
    "\n",
    "    # Return the generated sentence, excluding the start token\n",
    "    return ' '.join(sentence[1:])\n",
    "\n",
    "# Define a function to generate a word given a probability distribution\n",
    "def generate_word(model_prob, context=None):\n",
    "    # If no context is provided, use the full vocabulary and corresponding probabilities\n",
    "    if context is None:\n",
    "        words, probs = zip(*[(word, prob) for word, prob in model_prob.items()])\n",
    "    # If a context is provided, use only the vocabulary and probabilities corresponding to that context\n",
    "    else:\n",
    "        words, probs = zip(*[(word, model_prob.get(tuple(context + [word]), 0)) for word in vocab])\n",
    "        \n",
    "    # Cast the probabilities to a numpy array of float64 data type    \n",
    "    probs = np.array(probs, dtype=np.float64)\n",
    "    epsilon = 1e-10\n",
    "    probs = np.add(probs, epsilon)  # Add a small constant to the probabilities\n",
    "    probs /= probs.sum()\n",
    "    next_word = np.random.choice(words, p=probs)\n",
    "    return next_word\n",
    "\n",
    "# Ask the user for the model and phrase input\n",
    "model_input = input('\\nWhich language model would you like to use? (Vanilla, Laplace, UNK): ').lower()  # Convert to lowercase\n",
    "phrase_input = input('Please enter a phrase: ')\n",
    "\n",
    "# Generate the sentence using the selected model and input phrase\n",
    "generate_sentence(model_input, phrase_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be5f9d5",
   "metadata": {},
   "source": [
    "### Test output for Generation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9baea32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating Vanilla model...\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Unigram: this is a the cost i change fisk the top i know i the is little s rawsthorne shower\n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Bigram: this is a later one there to of amends relevant can if the that wa the reinforce partly he\n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Trigram: this is a gossip bk a mealy-mouthed for her social er the this this out at for key in\n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Generating Laplace model...\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Unigram: this is a s eye tagusage repeated for allowing of nobody instyle door n't been a be up that\n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Bigram: this is a awkward interlocutory s dc-10s always obtained do the might and she and targeting title presented this\n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Trigram: this is a are cup there will no me of the for whole all merger his the yet the\n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Generating UNK model...\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Unigram: this is a breast spoken in will madrid africa to and thereby in overlap and hall a saw bought\n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Bigram: this is a specimen much purely in to next what so understood body is is buying not water staff\n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Trigram: this is a could stunning in aldershot damme zonked ablated prized l999 bach gecko inside-reared septuagenarian streetlamps gator structurally\n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test the sentence generation function with different models and phrases\n",
    "models = ['Vanilla', 'Laplace', 'UNK']\n",
    "phrase = ['this is a']\n",
    "\n",
    "for model in models:\n",
    "    for sentence in phrase:\n",
    "        generate_sentence(model, sentence)\n",
    "        print(\"-\" * 123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16426ad7",
   "metadata": {},
   "source": [
    "## Sen_Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bcd7341a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence: lets go to the bank\n",
      "UNK bigram sentence probability: 1\n"
     ]
    }
   ],
   "source": [
    "def sen_probability(sentence, model_name, ngram_type):\n",
    "    # Tokenize sentence into words\n",
    "    sentence_tokens = sentence.lower().split()\n",
    "\n",
    "    # Initialize dictionaries to store sentence probabilities\n",
    "    sentence_probs = {\n",
    "        \"Vanilla\": {\"unigram\": 1, \"bigram\": None, \"trigram\": None},\n",
    "        \"Laplace\": {\"unigram\": 1, \"bigram\": None, \"trigram\": None},\n",
    "        \"UNK\": {\"unigram\": 1, \"bigram\": None, \"trigram\": None}\n",
    "    }\n",
    "\n",
    "    # Calculate probabilities for each model and n-gram type\n",
    "    for i in range(len(sentence_tokens)):\n",
    "        # Iterate through each model and corresponding probabilities\n",
    "        for model_name, model_probs in [(\"Vanilla\", (vanilla_unigram_probs, vanilla_bigram_probs, vanilla_trigram_probs)),\n",
    "                                        (\"Laplace\", (laplace_unigram_probs, laplace_bigram_probs, laplace_trigram_probs)),\n",
    "                                        (\"UNK\", (unk_laplace_unigram_probs, unk_laplace_bigram_probs, unk_laplace_trigram_probs))]:\n",
    "            # For the first word, calculate the unigram probability and multiply it by the running total\n",
    "            if i == 0:\n",
    "                sentence_probs[model_name][\"unigram\"] *= model_probs[0].get(sentence_tokens[i], 1)\n",
    "            # For the second word, calculate the bigram probability and multiply it by the running total\n",
    "            elif i == 1:\n",
    "                if sentence_probs[model_name][\"bigram\"] is None:\n",
    "                    sentence_probs[model_name][\"bigram\"] = 1\n",
    "                sentence_probs[model_name][\"bigram\"] *= model_probs[1].get((sentence_tokens[i-1], sentence_tokens[i]), 1)\n",
    "            # For all subsequent words, calculate the trigram probability and multiply it by the running total\n",
    "            else:\n",
    "                if sentence_probs[model_name][\"trigram\"] is None:\n",
    "                    sentence_probs[model_name][\"trigram\"] = 1\n",
    "                sentence_probs[model_name][\"trigram\"] *= model_probs[2].get((sentence_tokens[i-2], sentence_tokens[i-1], sentence_tokens[i]), 1)\n",
    "\n",
    "    # Print probabilities for the specified model and n-gram type\n",
    "    print(f\"\\nSentence: {sentence}\")\n",
    "    print(f\"{model_name} {ngram_type} sentence probability:\", sentence_probs[model_name][ngram_type])\n",
    "\n",
    "# Test the sen_probability function with a sample sentence, model, and n-gram type\n",
    "sentence = \"lets go to the bank\"\n",
    "model_name = \"Vanilla\"\n",
    "ngram_type = \"bigram\"\n",
    "\n",
    "prob = sen_probability(sentence, model_name, ngram_type)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
