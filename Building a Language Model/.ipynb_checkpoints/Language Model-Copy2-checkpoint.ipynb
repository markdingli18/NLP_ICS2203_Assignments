{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f87b502",
   "metadata": {},
   "source": [
    "# <u> ICS2203 – Statistical Natural Language Processing <u>\n",
    "## <u> Building a Language Model – Part I <u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6d96e4",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad830519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard library imports\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "# third-party library imports\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from spellchecker import SpellChecker\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952f74c8",
   "metadata": {},
   "source": [
    "## Extracting and Preprocessing the Selected Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fdbeb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['oxford', 'art', 'journal', 'sample', 'containing', '26310', 'word', 'periodical', 'domain', 'art', 'data', 'capture', 'transcription', 'oxford', 'university', 'press', 'bnc', 'xml', 'edition', 'december', '2006', '26310', 'token', '26548', 'w', 'unit', '1082', 'unit', 'distributed', 'licence', 'oxford', 'university', 'computing', 'service', 'behalf', 'bnc', 'consortium', 'material', 'protected', 'international', 'copyright', 'law', 'may', 'copied', 'redistributed', 'way', 'consult', 'bnc', 'web', 'site', 'full', 'licencing', 'distribution', 'condition', 'a6u', 'artjnl', 'oxford', 'art', 'journal', 'oxford', 'university', 'press', 'oxford', '1991', 'w', 'ac', 'humanity', 'art', 'art', 'tag', 'usage', 'updated', 'bnc', 'xml', 'last', 'check', 'bnc', 'world', 'first', 'release', 'redo', 'tagusage', 'table', 'check', 'tagcounts', 'resequenced', 'unit', 'added', 'header', 'added', 'date', 'info', 'updated', 'catrefs', 'updated', 'source', 'title', 'updated', 'title', 'corrected']\n",
      "Total size of corpus: 2093188 words\n",
      "Generation Time(HH::MM:SS:ms) - 0:00:20.412599\n",
      "\n",
      "\n",
      "Memory Use: 0.365845 GB\n"
     ]
    }
   ],
   "source": [
    "corpus_dir_path = 'corpus'\n",
    "tokens = []\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "generation_start = datetime.now()\n",
    "\n",
    "for root_dir, subdirs, files in os.walk(corpus_dir_path):\n",
    "    for filename in files:\n",
    "        if filename.endswith('.xml'):\n",
    "            tree = ET.parse(os.path.join(root_dir, filename))\n",
    "            root = tree.getroot()\n",
    "            text = ''\n",
    "            for element in root.iter():\n",
    "                if element.text is not None:\n",
    "                    text += element.text + ' '\n",
    "\n",
    "            text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # Remove URLs\n",
    "            text = text.lower()\n",
    "            file_tokens = tokenizer.tokenize(text)\n",
    "\n",
    "            # Remove stopwords and lemmatize tokens\n",
    "            file_tokens = [lemmatizer.lemmatize(token) for token in file_tokens if token not in stop_words]\n",
    "\n",
    "            tokens.extend(file_tokens)\n",
    "\n",
    "generation_end = datetime.now()\n",
    "generation_time = generation_end - generation_start\n",
    "\n",
    "def RAMusage():\n",
    "    pid = os.getpid()\n",
    "    py = psutil.Process(pid)\n",
    "    memoryUse = py.memory_info()[0] / 2.**30\n",
    "    return memoryUse\n",
    "\n",
    "# Output list of the first 100 tokens\n",
    "print(tokens[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffae0ee0",
   "metadata": {},
   "source": [
    "## Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67fe436e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of corpus: 2093188 words\n",
      "Generation Time(HH::MM:SS:ms) - 0:00:20.412599\n",
      "\n",
      "\n",
      "Memory Use: 0.362892 GB\n"
     ]
    }
   ],
   "source": [
    "print(\"Total size of corpus: \"+str(len(tokens))+\" words\")\n",
    "print('Generation Time(HH::MM:SS:ms) - {}\\n\\n'.format(generation_time))\n",
    "print(\"Memory Use: {:.6f} GB\".format(RAMusage()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22f0a11",
   "metadata": {},
   "source": [
    "## Split the corpus tokens into test set and train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54d92c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 1674550\n",
      "Test set size: 418638\n"
     ]
    }
   ],
   "source": [
    "# Split the words into train and test sets\n",
    "train_set, test_set = train_test_split(tokens, test_size=0.2)\n",
    "\n",
    "# Define the vocabulary from the training set\n",
    "vocab = set()\n",
    "for sentence in train_set:\n",
    "    for word in sentence.split():\n",
    "        vocab.add(word)\n",
    "vocab = list(vocab)\n",
    "vocab.sort()\n",
    "\n",
    "# Print the size of the train and test sets\n",
    "print(\"Train set size:\", len(train_set))\n",
    "print(\"Test set size:\", len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99525c2d",
   "metadata": {},
   "source": [
    "## Vanilla Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7200d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to create n-grams from tokens\n",
    "def create_vanilla_ngrams(tokens, n):\n",
    "    return [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Create vanilla unigram model\n",
    "vanilla_unigram_freq = {}\n",
    "vanilla_total_unigrams = len(train_set)\n",
    "for word in train_set:\n",
    "    if word in vanilla_unigram_freq:\n",
    "        vanilla_unigram_freq[word] += 1\n",
    "    else:\n",
    "        vanilla_unigram_freq[word] = 1\n",
    "\n",
    "# Calculate vanilla unigram probabilities\n",
    "vanilla_unigram_probs = {}\n",
    "for word, count in vanilla_unigram_freq.items():\n",
    "    vanilla_unigram_probs[word] = count / vanilla_total_unigrams\n",
    "    \n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Create vanilla bigram model\n",
    "vanilla_bigram_freq = {}\n",
    "vanilla_total_bigrams = len(create_vanilla_ngrams(train_set, 2))\n",
    "vanilla_bigram_tokens = create_vanilla_ngrams(train_set, 2)\n",
    "for tokens in vanilla_bigram_tokens:\n",
    "    if tokens in vanilla_bigram_freq:\n",
    "        vanilla_bigram_freq[tokens] += 1\n",
    "    else:\n",
    "        vanilla_bigram_freq[tokens] = 1\n",
    "\n",
    "# Calculate vanilla bigram probabilities\n",
    "vanilla_bigram_probs = {}\n",
    "for tokens, count in vanilla_bigram_freq.items():\n",
    "    vanilla_bigram_probs[tokens] = count / vanilla_unigram_freq[tokens[0]]\n",
    "    \n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Create vanilla trigram model\n",
    "vanilla_trigram_freq = {}\n",
    "vanilla_total_trigrams = len(create_vanilla_ngrams(train_set, 3))\n",
    "vanilla_trigram_tokens = create_vanilla_ngrams(train_set, 3)\n",
    "for tokens in vanilla_trigram_tokens:\n",
    "    if tokens in vanilla_trigram_freq:\n",
    "        vanilla_trigram_freq[tokens] += 1\n",
    "    else:\n",
    "        vanilla_trigram_freq[tokens] = 1\n",
    "\n",
    "# Calculate vanilla trigram probabilities\n",
    "vanilla_trigram_probs = {}\n",
    "for tokens, count in vanilla_trigram_freq.items():\n",
    "    vanilla_trigram_probs[tokens] = count / vanilla_bigram_freq[(tokens[0], tokens[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2896743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla unigram model (subset):\n",
      "{'cage': 35, 'could': 6133, 'forgotten': 155, 'house': 2053, 'show': 1177, 'scarcity': 12, 'second': 1502, 'exchequer': 14, 'round': 1601, 'tag': 191}\n",
      "\n",
      "Vanilla unigram probabilities (subset):\n",
      "{'cage': 2.090113761906184e-05, 'could': 0.0036624764862201787, 'forgotten': 9.256218088441671e-05, 'house': 0.001226001015198113, 'show': 0.0007028753993610224, 'scarcity': 7.166104326535487e-06, 'second': 0.0008969573915380252, 'exchequer': 8.360455047624735e-06, 'round': 0.0009560777522319429, 'tag': 0.00011406049386402317}\n",
      "\n",
      "Vanilla bigram model (subset):\n",
      "{('cage', 'could'): 1, ('could', 'forgotten'): 1, ('forgotten', 'house'): 1, ('house', 'show'): 1, ('show', 'scarcity'): 1, ('scarcity', 'second'): 1, ('second', 'exchequer'): 1, ('exchequer', 'round'): 1, ('round', 'tag'): 1, ('tag', 'beside'): 1}\n",
      "\n",
      "Vanilla bigram probabilities (subset):\n",
      "{('cage', 'could'): 0.02857142857142857, ('could', 'forgotten'): 0.00016305233980107615, ('forgotten', 'house'): 0.0064516129032258064, ('house', 'show'): 0.0004870920603994155, ('show', 'scarcity'): 0.0008496176720475786, ('scarcity', 'second'): 0.08333333333333333, ('second', 'exchequer'): 0.0006657789613848203, ('exchequer', 'round'): 0.07142857142857142, ('round', 'tag'): 0.0006246096189881324, ('tag', 'beside'): 0.005235602094240838}\n",
      "\n",
      "Vanilla trigram model (subset):\n",
      "{('cage', 'could', 'forgotten'): 1, ('could', 'forgotten', 'house'): 1, ('forgotten', 'house', 'show'): 1, ('house', 'show', 'scarcity'): 1, ('show', 'scarcity', 'second'): 1, ('scarcity', 'second', 'exchequer'): 1, ('second', 'exchequer', 'round'): 1, ('exchequer', 'round', 'tag'): 1, ('round', 'tag', 'beside'): 1, ('tag', 'beside', 'book'): 1}\n",
      "\n",
      "Vanilla trigram probabilities (subset):\n",
      "{('cage', 'could', 'forgotten'): 1.0, ('could', 'forgotten', 'house'): 1.0, ('forgotten', 'house', 'show'): 1.0, ('house', 'show', 'scarcity'): 1.0, ('show', 'scarcity', 'second'): 1.0, ('scarcity', 'second', 'exchequer'): 1.0, ('second', 'exchequer', 'round'): 1.0, ('exchequer', 'round', 'tag'): 1.0, ('round', 'tag', 'beside'): 1.0, ('tag', 'beside', 'book'): 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Testing Vanilla Language Model\n",
    "print(\"Vanilla unigram model (subset):\")\n",
    "print({k: vanilla_unigram_freq[k] for k in list(vanilla_unigram_freq)[:10]})\n",
    "print(\"\\nVanilla unigram probabilities (subset):\")\n",
    "print({k: vanilla_unigram_probs[k] for k in list(vanilla_unigram_probs)[:10]})\n",
    "print(\"\\nVanilla bigram model (subset):\")\n",
    "print({k: vanilla_bigram_freq[k] for k in list(vanilla_bigram_freq)[:10]})\n",
    "print(\"\\nVanilla bigram probabilities (subset):\")\n",
    "print({k: vanilla_bigram_probs[k] for k in list(vanilla_bigram_probs)[:10]})\n",
    "print(\"\\nVanilla trigram model (subset):\")\n",
    "print({k: vanilla_trigram_freq[k] for k in list(vanilla_trigram_freq)[:10]})\n",
    "print(\"\\nVanilla trigram probabilities (subset):\")\n",
    "print({k: vanilla_trigram_probs[k] for k in list(vanilla_trigram_probs)[:10]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a99719",
   "metadata": {},
   "source": [
    "## Laplace Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cab7ff62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to create n-grams from tokens\n",
    "def create_laplace_ngrams(tokens, n):\n",
    "    return [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Create Laplace unigram model\n",
    "laplace_unigram_freq = {}\n",
    "laplace_total_unigrams = len(train_set)\n",
    "for word in train_set:\n",
    "    if word in laplace_unigram_freq:\n",
    "        laplace_unigram_freq[word] += 1\n",
    "    else:\n",
    "        laplace_unigram_freq[word] = 1\n",
    "\n",
    "# Calculate Laplace unigram probabilities\n",
    "laplace_unigram_probs = {}\n",
    "for word, count in laplace_unigram_freq.items():\n",
    "    laplace_unigram_probs[word] = (count + 1) / (laplace_total_unigrams + len(laplace_unigram_freq))\n",
    "    \n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "# Create Laplace bigram model\n",
    "laplace_bigram_freq = {}\n",
    "laplace_total_bigrams = len(create_laplace_ngrams(train_set, 2))\n",
    "laplace_bigram_tokens = create_laplace_ngrams(train_set, 2)\n",
    "for tokens in laplace_bigram_tokens:\n",
    "    if tokens in laplace_bigram_freq:\n",
    "        laplace_bigram_freq[tokens] += 1\n",
    "    else:\n",
    "        laplace_bigram_freq[tokens] = 1\n",
    "\n",
    "# Calculate Laplace bigram probabilities\n",
    "laplace_bigram_probs = {}\n",
    "for tokens, count in laplace_bigram_freq.items():\n",
    "    laplace_bigram_probs[tokens] = (count + 1) / (laplace_unigram_freq[tokens[0]] + len(laplace_unigram_freq))\n",
    "    \n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "# Create Laplace trigram model\n",
    "laplace_trigram_freq = {}\n",
    "laplace_total_trigrams = len(create_laplace_ngrams(train_set, 3))\n",
    "laplace_trigram_tokens = create_laplace_ngrams(train_set, 3)\n",
    "for tokens in laplace_trigram_tokens:\n",
    "    if tokens in laplace_trigram_freq:\n",
    "        laplace_trigram_freq[tokens] += 1\n",
    "    else:\n",
    "        laplace_trigram_freq[tokens] = 1\n",
    "\n",
    "# Calculate Laplace trigram probabilities\n",
    "laplace_trigram_probs = {}\n",
    "for tokens, count in laplace_trigram_freq.items():\n",
    "    laplace_trigram_probs[tokens] = (count + 1) / (laplace_bigram_freq[(tokens[0], tokens[1])] + len(laplace_unigram_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f32d1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Laplace unigram model (subset):\n",
      "{'cage': 35, 'could': 6133, 'forgotten': 155, 'house': 2053, 'show': 1177, 'scarcity': 12, 'second': 1502, 'exchequer': 14, 'round': 1601, 'tag': 191}\n",
      "\n",
      "Laplace unigram probabilities (subset):\n",
      "{'cage': 2.078286752769317e-05, 'could': 0.0035411697059686085, 'forgotten': 9.005909262000373e-05, 'house': 0.0011857780528300493, 'show': 0.0006800616096561821, 'scarcity': 7.5049243850003115e-06, 'second': 0.0008676847192811899, 'exchequer': 8.659528136538822e-06, 'round': 0.0009248376049823461, 'tag': 0.00011084196014769691}\n",
      "\n",
      "Laplace bigram model (subset):\n",
      "{('cage', 'could'): 1, ('could', 'forgotten'): 1, ('forgotten', 'house'): 1, ('house', 'show'): 1, ('show', 'scarcity'): 1, ('scarcity', 'second'): 1, ('second', 'exchequer'): 1, ('exchequer', 'round'): 1, ('round', 'tag'): 1, ('tag', 'beside'): 1}\n",
      "\n",
      "Laplace bigram probabilities (subset):\n",
      "{('cage', 'could'): 3.467346266534908e-05, ('could', 'forgotten'): 3.1358284074695435e-05, ('forgotten', 'house'): 3.4601477483088526e-05, ('house', 'show'): 3.350139868339503e-05, ('show', 'scarcity'): 3.4000306002754026e-05, ('scarcity', 'second'): 3.468729404419161e-05, ('second', 'exchequer'): 3.381348481774532e-05, ('exchequer', 'round'): 3.4686090877558096e-05, ('round', 'tag'): 3.375698347595659e-05, ('tag', 'beside'): 3.4579940176703494e-05}\n",
      "\n",
      "Laplace trigram model (subset):\n",
      "{('cage', 'could', 'forgotten'): 1, ('could', 'forgotten', 'house'): 1, ('forgotten', 'house', 'show'): 1, ('house', 'show', 'scarcity'): 1, ('show', 'scarcity', 'second'): 1, ('scarcity', 'second', 'exchequer'): 1, ('second', 'exchequer', 'round'): 1, ('exchequer', 'round', 'tag'): 1, ('round', 'tag', 'beside'): 1, ('tag', 'beside', 'book'): 1}\n",
      "\n",
      "Laplace trigram probabilities (subset):\n",
      "{('cage', 'could', 'forgotten'): 3.4693912952972404e-05, ('could', 'forgotten', 'house'): 3.4693912952972404e-05, ('forgotten', 'house', 'show'): 3.4693912952972404e-05, ('house', 'show', 'scarcity'): 3.4693912952972404e-05, ('show', 'scarcity', 'second'): 3.4693912952972404e-05, ('scarcity', 'second', 'exchequer'): 3.4693912952972404e-05, ('second', 'exchequer', 'round'): 3.4693912952972404e-05, ('exchequer', 'round', 'tag'): 3.4693912952972404e-05, ('round', 'tag', 'beside'): 3.4693912952972404e-05, ('tag', 'beside', 'book'): 3.4693912952972404e-05}\n"
     ]
    }
   ],
   "source": [
    "# Testing Laplace Language Model\n",
    "print(\"Laplace unigram model (subset):\")\n",
    "print({k: laplace_unigram_freq[k] for k in list(laplace_unigram_freq)[:10]})\n",
    "print(\"\\nLaplace unigram probabilities (subset):\")\n",
    "print({k: laplace_unigram_probs[k] for k in list(laplace_unigram_probs)[:10]})\n",
    "print(\"\\nLaplace bigram model (subset):\")\n",
    "print({k: laplace_bigram_freq[k] for k in list(laplace_bigram_freq)[:10]})\n",
    "print(\"\\nLaplace bigram probabilities (subset):\")\n",
    "print({k: laplace_bigram_probs[k] for k in list(laplace_bigram_probs)[:10]})\n",
    "print(\"\\nLaplace trigram model (subset):\")\n",
    "print({k: laplace_trigram_freq[k] for k in list(laplace_trigram_freq)[:10]})\n",
    "print(\"\\nLaplace trigram probabilities (subset):\")\n",
    "print({k: laplace_trigram_probs[k] for k in list(laplace_trigram_probs)[:10]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11b3ff7",
   "metadata": {},
   "source": [
    "## UNK Language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c4c2c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new list with <UNK> tokens for words with count less than or equal to 2\n",
    "unk_train_set = []\n",
    "word_freq = {}\n",
    "for word in train_set:\n",
    "    if word in word_freq:\n",
    "        word_freq[word] += 1\n",
    "    else:\n",
    "        word_freq[word] = 1\n",
    "    if word_freq[word] <= 2:\n",
    "        unk_train_set.append('<UNK>')\n",
    "    else:\n",
    "        unk_train_set.append(word)\n",
    "\n",
    "# Create UNK Laplace unigram model\n",
    "unk_laplace_unigram_freq = {}\n",
    "unk_laplace_total_unigrams = len(unk_train_set)\n",
    "for word in unk_train_set:\n",
    "    if word in unk_laplace_unigram_freq:\n",
    "        unk_laplace_unigram_freq[word] += 1\n",
    "    else:\n",
    "        unk_laplace_unigram_freq[word] = 1\n",
    "\n",
    "# Calculate UNK Laplace unigram probabilities\n",
    "unk_laplace_unigram_probs = {}\n",
    "for word, count in unk_laplace_unigram_freq.items():\n",
    "    unk_laplace_unigram_probs[word] = (count + 1) / (unk_laplace_total_unigrams + len(unk_laplace_unigram_freq))\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Create UNK Laplace bigram model\n",
    "unk_laplace_bigram_freq = {}\n",
    "unk_laplace_total_bigrams = len(create_laplace_ngrams(unk_train_set, 2))\n",
    "unk_laplace_bigram_tokens = create_laplace_ngrams(unk_train_set, 2)\n",
    "for tokens in unk_laplace_bigram_tokens:\n",
    "    if tokens in unk_laplace_bigram_freq:\n",
    "        unk_laplace_bigram_freq[tokens] += 1\n",
    "    else:\n",
    "        unk_laplace_bigram_freq[tokens] = 1\n",
    "\n",
    "# Calculate UNK Laplace bigram probabilities\n",
    "unk_laplace_bigram_probs = {}\n",
    "for tokens, count in unk_laplace_bigram_freq.items():\n",
    "    unk_laplace_bigram_probs[tokens] = (count + 1) / (unk_laplace_unigram_freq[tokens[0]] + len(unk_laplace_unigram_freq))\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Create UNK Laplace trigram model\n",
    "unk_laplace_trigram_freq = {}\n",
    "unk_laplace_total_trigrams = len(create_laplace_ngrams(unk_train_set, 3))\n",
    "unk_laplace_trigram_tokens = create_laplace_ngrams(unk_train_set, 3)\n",
    "for tokens in unk_laplace_trigram_tokens:\n",
    "    if tokens in unk_laplace_trigram_freq:\n",
    "        unk_laplace_trigram_freq[tokens] += 1\n",
    "    else:\n",
    "        unk_laplace_trigram_freq[tokens] = 1\n",
    "\n",
    "# Calculate UNK Laplace trigram probabilities\n",
    "unk_laplace_trigram_probs = {}\n",
    "for tokens, count in unk_laplace_trigram_freq.items():\n",
    "    unk_laplace_trigram_probs[tokens] = (count + 1) / (unk_laplace_bigram_freq[(tokens[0], tokens[1])] + len(unk_laplace_unigram_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68ca8b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNK Laplace unigram model (subset):\n",
      "{'<UNK>': 93967, 'well': 9356, 'let': 1643, 'house': 2051, 'day': 2934, 'said': 10439, 'would': 8178, 'could': 6131, 'one': 13665, 'home': 2667}\n",
      "\n",
      "UNK Laplace unigram probabilities (subset):\n",
      "{'<UNK>': 0.05517494063738189, 'well': 0.0054941248035925245, 'let': 0.000965303107524432, 'house': 0.00120486738238451, 'day': 0.0017233361439076692, 'said': 0.006130027033184349, 'would': 0.004802441676668084, 'could': 0.0036005101309852902, 'one': 0.008024228873131927, 'home': 0.0015665624640360005}\n",
      "\n",
      "UNK Laplace bigram model (subset):\n",
      "{('<UNK>', '<UNK>'): 12144, ('<UNK>', 'well'): 524, ('well', '<UNK>'): 533, ('<UNK>', 'let'): 102, ('let', '<UNK>'): 99, ('<UNK>', 'house'): 105, ('house', '<UNK>'): 122, ('<UNK>', 'day'): 147, ('day', '<UNK>'): 156, ('<UNK>', 'said'): 591}\n",
      "\n",
      "UNK Laplace bigram probabilities (subset):\n",
      "{('<UNK>', '<UNK>'): 0.0991355737129517, ('<UNK>', 'well'): 0.0042853994400411394, ('well', '<UNK>'): 0.014090453322075043, ('<UNK>', 'let'): 0.0008407545568080712, ('let', '<UNK>'): 0.003312903760145768, ('<UNK>', 'house'): 0.0008652425536083063, ('house', '<UNK>'): 0.004020527571666721, ('<UNK>', 'day'): 0.0012080745088115974, ('day', '<UNK>'): 0.004987927309696277, ('<UNK>', 'said'): 0.00483229803524639}\n",
      "\n",
      "UNK Laplace trigram model (subset):\n",
      "{('<UNK>', '<UNK>', '<UNK>'): 3917, ('<UNK>', '<UNK>', 'well'): 61, ('<UNK>', 'well', '<UNK>'): 66, ('well', '<UNK>', '<UNK>'): 68, ('<UNK>', '<UNK>', 'let'): 14, ('<UNK>', 'let', '<UNK>'): 8, ('let', '<UNK>', '<UNK>'): 18, ('<UNK>', '<UNK>', 'house'): 13, ('<UNK>', 'house', '<UNK>'): 15, ('house', '<UNK>', '<UNK>'): 16}\n",
      "\n",
      "UNK Laplace trigram probabilities (subset):\n",
      "{('<UNK>', '<UNK>', '<UNK>'): 0.09629848104999263, ('<UNK>', '<UNK>', 'well'): 0.0015238657031902865, ('<UNK>', 'well', '<UNK>'): 0.0023050987407968074, ('well', '<UNK>', '<UNK>'): 0.0023731728288907995, ('<UNK>', '<UNK>', 'let'): 0.0003686771862557145, ('<UNK>', 'let', '<UNK>'): 0.0003142019271051529, ('let', '<UNK>', '<UNK>'): 0.0006633846583569009, ('<UNK>', '<UNK>', 'house'): 0.0003440987071720002, ('<UNK>', 'house', '<UNK>'): 0.0005585227074388243, ('house', '<UNK>', '<UNK>'): 0.0005930784259000837}\n"
     ]
    }
   ],
   "source": [
    "# Testing UNK Language Model\n",
    "print(\"UNK Laplace unigram model (subset):\")\n",
    "print({k: unk_laplace_unigram_freq[k] for k in list(unk_laplace_unigram_freq)[:10]})\n",
    "print(\"\\nUNK Laplace unigram probabilities (subset):\")\n",
    "print({k: unk_laplace_unigram_probs[k] for k in list(unk_laplace_unigram_probs)[:10]})\n",
    "print(\"\\nUNK Laplace bigram model (subset):\")\n",
    "print({k: unk_laplace_bigram_freq[k] for k in list(unk_laplace_bigram_freq)[:10]})\n",
    "print(\"\\nUNK Laplace bigram probabilities (subset):\")\n",
    "print({k: unk_laplace_bigram_probs[k] for k in list(unk_laplace_bigram_probs)[:10]})\n",
    "print(\"\\nUNK Laplace trigram model (subset):\")\n",
    "print({k: unk_laplace_trigram_freq[k] for k in list(unk_laplace_trigram_freq)[:10]})\n",
    "print(\"\\nUNK Laplace trigram probabilities (subset):\")\n",
    "print({k: unk_laplace_trigram_probs[k] for k in list(unk_laplace_trigram_probs)[:10]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed85418",
   "metadata": {},
   "source": [
    "## Linear Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1395f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_interpolation(sentence, lm_type):\n",
    "    # Set the lambda values for each n-gram model\n",
    "    lambda_3 = 0.6\n",
    "    lambda_2 = 0.3\n",
    "    lambda_1 = 0.1\n",
    "    \n",
    "    # Tokenize the sentence\n",
    "    sentence_tokens = sentence.split()\n",
    "    \n",
    "    # Calculate the probabilities of each n-gram model for the sentence\n",
    "    if lm_type == \"vanilla\":\n",
    "        unigram_probs = [vanilla_unigram_probs[token] for token in sentence_tokens]\n",
    "        bigram_probs = [vanilla_bigram_probs.get((sentence_tokens[i-1], sentence_tokens[i]), 0) for i in range(1, len(sentence_tokens))]\n",
    "        trigram_probs = [vanilla_trigram_probs.get((sentence_tokens[i-2], sentence_tokens[i-1], sentence_tokens[i]), 0) for i in range(2, len(sentence_tokens))]\n",
    "    elif lm_type == \"laplace\":\n",
    "        unigram_probs = [(laplace_unigram_freq.get(token, 0) + 1) / (laplace_total_unigrams + len(laplace_unigram_freq)) for token in sentence_tokens]\n",
    "        bigram_probs = [(laplace_bigram_freq.get((sentence_tokens[i-1], sentence_tokens[i]), 0) + 1) / (laplace_unigram_freq.get(sentence_tokens[i-1], 0) + len(laplace_unigram_freq)) for i in range(1, len(sentence_tokens))]\n",
    "        trigram_probs = [(laplace_trigram_freq.get((sentence_tokens[i-2], sentence_tokens[i-1], sentence_tokens[i]), 0) + 1) / (laplace_bigram_freq.get((sentence_tokens[i-2], sentence_tokens[i-1]), 0) + len(laplace_unigram_freq)) for i in range(2, len(sentence_tokens))]\n",
    "    elif lm_type == \"unk\":\n",
    "        unk_sentence_tokens = ['<UNK>' if word_freq[token] <= 2 else token for token in sentence_tokens]\n",
    "        unk_laplace_unigram_probs = {word: (count + 1) / (unk_laplace_total_unigrams + len(unk_laplace_unigram_freq)) for word, count in unk_laplace_unigram_freq.items()}\n",
    "        unk_laplace_bigram_probs = {(word1, word2): (count + 1) / (unk_laplace_unigram_freq.get(word1, 0) + len(unk_laplace_unigram_freq)) for (word1, word2), count in unk_laplace_bigram_freq.items()}\n",
    "        unk_laplace_trigram_probs = {(word1, word2, word3): (count + 1) / (unk_laplace_bigram_freq.get((word1, word2), 0) + len(unk_laplace_unigram_freq)) for (word1, word2, word3), count in unk_laplace_trigram_freq.items()}\n",
    "        unk_laplace_trigram_probs = {(word1, word2, word3): (count + 1) / (unk_laplace_bigram_freq.get((word1, word2), 0) + len(unk_laplace_unigram_freq)) for (word1, word2, word3), count in unk_laplace_trigram_freq.items()}\n",
    "        unigram_probs = [unk_laplace_unigram_probs.get(token, 1 / (unk_laplace_total_unigrams + len(unk_laplace_unigram_freq))) for token in unk_sentence_tokens]\n",
    "        bigram_probs = [(unk_laplace_bigram_freq.get((unk_sentence_tokens[i-1], unk_sentence_tokens[i]), 0) + 1) / (unk_laplace_unigram_freq.get(unk_sentence_tokens[i-1], 0) + len(unk_laplace_unigram_freq)) for i in range(1, len(unk_sentence_tokens))]\n",
    "        trigram_probs = [(unk_laplace_trigram_probs.get((unk_sentence_tokens[i-2], unk_sentence_tokens[i-1], unk_sentence_tokens[i]), 0) + 1) / (unk_laplace_bigram_freq.get((unk_sentence_tokens[i-2], unk_sentence_tokens[i-1]), 0) + len(unk_laplace_unigram_freq)) for i in range(2, len(unk_sentence_tokens))]\n",
    "        \n",
    "    # Calculate the probability of the sentence using linear interpolation\n",
    "    sentence_prob = 1\n",
    "    for i in range(len(sentence_tokens)):\n",
    "        if i == 0:\n",
    "            sentence_prob *= unigram_probs[i]**lambda_1\n",
    "        elif i == 1:\n",
    "            sentence_prob *= (lambda_2*bigram_probs[i-1] + lambda_1*unigram_probs[i])**lambda_1\n",
    "        else:\n",
    "            sentence_prob *= (lambda_3*trigram_probs[i-2] + lambda_2*bigram_probs[i-1] + lambda_1*unigram_probs[i])**lambda_1\n",
    "\n",
    "    return sentence_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3bc5f6",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "965bd2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through sentences in test set\n",
    "for sentence in test_set:\n",
    "    # Tokenize sentence into words\n",
    "    sentence_tokens = sentence.lower().split()\n",
    "\n",
    "    # Initialize dictionaries to store sentence probabilities\n",
    "    sentence_probs = {\n",
    "        \"Vanilla\": {\"unigram\": 1, \"bigram\": None, \"trigram\": None},\n",
    "        \"Laplace\": {\"unigram\": 1, \"bigram\": None, \"trigram\": None},\n",
    "        \"UNK\": {\"unigram\": 1, \"bigram\": None, \"trigram\": None}\n",
    "    }\n",
    "\n",
    "    # Calculate probabilities for each model and n-gram type\n",
    "    for i in range(len(sentence_tokens)):\n",
    "        for model_name, model_probs in [(\"Vanilla\", (vanilla_unigram_probs, vanilla_bigram_probs, vanilla_trigram_probs)),\n",
    "                                        (\"Laplace\", (laplace_unigram_probs, laplace_bigram_probs, laplace_trigram_probs)),\n",
    "                                        (\"UNK\", (unk_laplace_unigram_probs, unk_laplace_bigram_probs, unk_laplace_trigram_probs))]:\n",
    "            if i == 0:\n",
    "                sentence_probs[model_name][\"unigram\"] *= model_probs[0].get(sentence_tokens[i], 1)\n",
    "            elif i == 1:\n",
    "                if sentence_probs[model_name][\"bigram\"] is None:\n",
    "                    sentence_probs[model_name][\"bigram\"] = 1\n",
    "                sentence_probs[model_name][\"bigram\"] *= model_probs[1].get((sentence_tokens[i-1], sentence_tokens[i]), 1)\n",
    "            else:\n",
    "                if sentence_probs[model_name][\"trigram\"] is None:\n",
    "                    sentence_probs[model_name][\"trigram\"] = 1\n",
    "                sentence_probs[model_name][\"trigram\"] *= model_probs[2].get((sentence_tokens[i-2], sentence_tokens[i-1], sentence_tokens[i]), 1)\n",
    "\n",
    "    # Print probabilities for each model for current sentence\n",
    "    #print(\"Sentence:\", sentence)\n",
    "    #for model_name in sentence_probs:\n",
    "    #    print(f\"{model_name} unigram sentence probability:\", sentence_probs[model_name][\"unigram\"])\n",
    "    #    if sentence_probs[model_name][\"bigram\"] is not None:\n",
    "    #        print(f\"{model_name} bigram sentence probability:\", sentence_probs[model_name][\"bigram\"])\n",
    "    #    else:\n",
    "    #        print(f\"{model_name} bigram sentence probability: N/A\")\n",
    "    #    if sentence_probs[model_name][\"trigram\"] is not None:\n",
    "    #        print(f\"{model_name} trigram sentence probability:\", sentence_probs[model_name][\"trigram\"])\n",
    "    #    else:\n",
    "    #        print(f\"{model_name} trigram sentence probability: N/A\")\n",
    "    #print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f2313b",
   "metadata": {},
   "source": [
    "## Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aedd4a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model           Unigram         Bigram          Trigram        \n",
      "------------------------------------------------------------\n",
      "Vanilla         335.19          3.08            1.00           \n",
      "Laplace         337.23          5.38            1.01           \n",
      "UNK             292.09          4.76            1.01           \n"
     ]
    }
   ],
   "source": [
    "def calculate_perplexity(test_set, lm_probs, ngram_type):\n",
    "    N = len(test_set)\n",
    "    log_prob_sum = 0\n",
    "\n",
    "    for i in range(len(test_set)):\n",
    "        if ngram_type == \"unigram\":\n",
    "            log_prob_sum += math.log(lm_probs.get(test_set[i], 1))\n",
    "        elif ngram_type == \"bigram\" and i > 0:\n",
    "            log_prob_sum += math.log(lm_probs.get((test_set[i-1], test_set[i]), 1))\n",
    "        elif ngram_type == \"trigram\" and i > 1:\n",
    "            log_prob_sum += math.log(lm_probs.get((test_set[i-2], test_set[i-1], test_set[i]), 1))\n",
    "\n",
    "    entropy = -(log_prob_sum / N)\n",
    "    perplexity = math.pow(2, entropy)\n",
    "    return perplexity\n",
    "\n",
    "# Calculate perplexities for each model and n-gram\n",
    "vanilla_unigram_perplexity = calculate_perplexity(test_set, vanilla_unigram_probs, \"unigram\")\n",
    "vanilla_bigram_perplexity = calculate_perplexity(test_set, vanilla_bigram_probs, \"bigram\")\n",
    "vanilla_trigram_perplexity = calculate_perplexity(test_set, vanilla_trigram_probs, \"trigram\")\n",
    "\n",
    "laplace_unigram_perplexity = calculate_perplexity(test_set, laplace_unigram_probs, \"unigram\")\n",
    "laplace_bigram_perplexity = calculate_perplexity(test_set, laplace_bigram_probs, \"bigram\")\n",
    "laplace_trigram_perplexity = calculate_perplexity(test_set, laplace_trigram_probs, \"trigram\")\n",
    "\n",
    "unk_unigram_perplexity = calculate_perplexity(test_set, unk_laplace_unigram_probs, \"unigram\")\n",
    "unk_bigram_perplexity = calculate_perplexity(test_set, unk_laplace_bigram_probs, \"bigram\")\n",
    "unk_trigram_perplexity = calculate_perplexity(test_set, unk_laplace_trigram_probs, \"trigram\")\n",
    "\n",
    "# Print the results in a table format\n",
    "print(\"{:<15} {:<15} {:<15} {:<15}\".format(\"Model\", \"Unigram\", \"Bigram\", \"Trigram\"))\n",
    "print(\"-\" * 60)\n",
    "print(\"{:<15} {:<15.2f} {:<15.2f} {:<15.2f}\".format(\"Vanilla\", vanilla_unigram_perplexity, vanilla_bigram_perplexity, vanilla_trigram_perplexity))\n",
    "print(\"{:<15} {:<15.2f} {:<15.2f} {:<15.2f}\".format(\"Laplace\", laplace_unigram_perplexity, laplace_bigram_perplexity, laplace_trigram_perplexity))\n",
    "print(\"{:<15} {:<15.2f} {:<15.2f} {:<15.2f}\".format(\"UNK\", unk_unigram_perplexity, unk_bigram_perplexity, unk_trigram_perplexity))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a94695",
   "metadata": {},
   "source": [
    "## Genetarion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83f6d118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which language model would you like to use? (Vanilla, Laplace, UNK): Vanilla\n",
      "Please enter a phrase: they all went\n",
      "Generating Vanilla model...\n",
      "GENERATED VANILLA SENTENCES:\n",
      "Unigram: they all went er hall funny princess cut data political red plaintiff preece open ireland darlington looked consent location\n",
      "Bigram: they all went equal love arsenal u heard dodgy avoiding different unit solve want especially mum employer rookie school\n",
      "Trigram: they all went queueing 22lb demoiselle turban disintegrated ramos alistair unladen encrusting crenistria valdez fossil fart offer chuba 191\n"
     ]
    }
   ],
   "source": [
    "def generate_sentence(model, phrase):\n",
    "    print(f\"Generating {model} model...\")\n",
    "    print(\"GENERATED\", model.upper(), \"SENTENCES:\")\n",
    "    for ngram_type in [\"unigram\", \"bigram\", \"trigram\"]:\n",
    "        generated_sentence = generate_ngram_sentence(model, phrase, ngram_type)\n",
    "        print(f\"{ngram_type.capitalize()}: {generated_sentence}\")\n",
    "\n",
    "def generate_ngram_sentence(model, phrase, ngram_type):\n",
    "    start_token = '<s>'\n",
    "    end_token = '</s>'\n",
    "    max_length = 20\n",
    "    sentence = [start_token] + phrase.split()\n",
    "    model_probs = None\n",
    "    \n",
    "    if model == 'Vanilla':\n",
    "        model_probs = [vanilla_unigram_probs, vanilla_bigram_probs, vanilla_trigram_probs]\n",
    "    elif model == 'Laplace':\n",
    "        model_probs = [laplace_unigram_probs, laplace_bigram_probs, laplace_trigram_probs]\n",
    "    elif model == 'UNK':\n",
    "        model_probs = [unk_laplace_unigram_probs, unk_laplace_bigram_probs, unk_laplace_trigram_probs]\n",
    "    else:\n",
    "        print('Invalid model selection')\n",
    "        return\n",
    "\n",
    "    while sentence[-1] != end_token and len(sentence) < max_length:\n",
    "        if ngram_type == \"unigram\":\n",
    "            next_word = generate_word(model_probs[0])\n",
    "        elif ngram_type == \"bigram\":\n",
    "            next_word = generate_word(model_probs[1], sentence[-1:])\n",
    "        elif ngram_type == \"trigram\":\n",
    "            if len(sentence) < 2:\n",
    "                next_word = generate_word(model_probs[1], sentence[-1:])\n",
    "            else:\n",
    "                next_word = generate_word(model_probs[2], sentence[-2:])\n",
    "        sentence.append(next_word)\n",
    "\n",
    "    return ' '.join(sentence[1:])\n",
    "\n",
    "def generate_word(model_prob, context=None):\n",
    "    if context is None:\n",
    "        words, probs = zip(*[(word, prob) for word, prob in model_prob.items()])\n",
    "    else:\n",
    "        words, probs = zip(*[(word, model_prob.get(tuple(context + [word]), 0)) for word in vocab])\n",
    "\n",
    "    probs = np.array(probs, dtype=np.float64)  # Cast to float64 data type\n",
    "    epsilon = 1e-10\n",
    "    probs = np.add(probs, epsilon)  # Add a small constant to the probabilities\n",
    "    probs /= probs.sum()\n",
    "    next_word = np.random.choice(words, p=probs)\n",
    "    return next_word\n",
    "\n",
    "# Ask the user for the model and phrase input\n",
    "model_input = input('Which language model would you like to use? (Vanilla, Laplace, UNK): ')\n",
    "phrase_input = input('Please enter a phrase: ')\n",
    "\n",
    "# Generate the sentence using the selected model and input phrase\n",
    "generate_sentence(model_input, phrase_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9baea32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the sentence generation function with different models and phrases\n",
    "#models = ['Vanilla', 'Laplace', 'UNK']\n",
    "#phrases = ['this is a', 'i love', 'the weather is']\n",
    "#\n",
    "#for model in models:\n",
    "#    for phrase in phrases:\n",
    "#        generate_sentence(model, phrase)\n",
    "#        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16426ad7",
   "metadata": {},
   "source": [
    "## Sen_Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bcd7341a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of the sentence 'this is an example sentence' using the Vanilla model is 0.0000000000\n",
      "The probability of the sentence 'this is an example sentence' using the Laplace model is 0.0000000000\n",
      "The probability of the sentence 'this is an example sentence' using the UNK model is 0.0000000000\n"
     ]
    }
   ],
   "source": [
    "def sen_probability(sentence, model):\n",
    "    unigram_probs, bigram_probs, trigram_probs = None, None, None\n",
    "    if model == 'Vanilla':\n",
    "        unigram_probs, bigram_probs, trigram_probs = vanilla_unigram_probs, vanilla_bigram_probs, vanilla_trigram_probs\n",
    "    elif model == 'Laplace':\n",
    "        unigram_probs, bigram_probs, trigram_probs = laplace_unigram_probs, laplace_bigram_probs, laplace_trigram_probs\n",
    "    elif model == 'UNK':\n",
    "        unigram_probs, bigram_probs, trigram_probs = unk_laplace_unigram_probs, unk_laplace_bigram_probs, unk_laplace_trigram_probs\n",
    "    else:\n",
    "        print('Invalid model selection')\n",
    "        return\n",
    "\n",
    "    tokens = ['<s>'] + sentence.split() + ['</s>']\n",
    "    prob = 1.0\n",
    "\n",
    "    for i in range(1, len(tokens)):\n",
    "        if i == 1:\n",
    "            prob *= unigram_probs.get(tokens[i], 0)\n",
    "        else:\n",
    "            bigram = (tokens[i-1], tokens[i])\n",
    "            bigram_prob = bigram_probs.get(bigram, 0)\n",
    "\n",
    "            if i >= 2:\n",
    "                trigram = (tokens[i-2], tokens[i-1], tokens[i])\n",
    "                trigram_prob = trigram_probs.get(trigram, 0)\n",
    "\n",
    "                # Apply linear interpolation\n",
    "                lambda1, lambda2, lambda3 = 1/3, 1/3, 1/3\n",
    "                prob *= (lambda1 * unigram_probs.get(tokens[i], 0) + lambda2 * bigram_prob + lambda3 * trigram_prob)\n",
    "\n",
    "    return prob\n",
    "\n",
    "## Example usage\n",
    "#models = ['Vanilla', 'Laplace', 'UNK']\n",
    "#sentence_input = 'this is an example sentence'\n",
    "#\n",
    "#for model in models:\n",
    "#    probability = sen_probability(sentence_input, model)\n",
    "#    print(f\"The probability of the sentence '{sentence_input}' using the {model} model is {probability:.10f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
